{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpSPUgHnLGAK"
      },
      "outputs": [],
      "source": [
        "CIDADES = {\n",
        "    \"Touba\": {\n",
        "        \"arquivo\": \"solar-measurementssenegal-toubaifcqc.csv\",\n",
        "        \"tipo\": \"wb\",\n",
        "        \"targets\": [\"dhi_rsi\", \"ghi_pyr\", \"ghi_sil\"],\n",
        "        \"ignorar\": [\"dhi_rsi\", \"ghi_pyr\", \"ghi_sil\"],\n",
        "        \"neurons\": [[50, 50], [100, 200], [100, 50]],\n",
        "        \"dropouts\": [[0.25, 0.25], [0.25, 0.25], [0, 0]],\n",
        "        \"epochs\": [20, 40, 30],\n",
        "        \"batch_size\": [128, 128, 128]\n",
        "    },\n",
        "\n",
        "    \"Fatick\": {\n",
        "        \"arquivo\": \"solar-measurementssenegal-fatickifcqc.csv\",\n",
        "        \"tipo\": \"wb\",\n",
        "        \"targets\": [\"dhi_rsi\", \"ghi_pyr\", \"ghi_sil\"],\n",
        "        \"ignorar\": [\"dhi_rsi\", \"ghi_pyr\", \"ghi_sil\"],\n",
        "        \"neurons\": [[100, 50], [100, 50], [50, 50]],\n",
        "        \"dropouts\": [[0.25, 0.25], [0.0, 0.25], [0.25, 0.25]],\n",
        "        \"epochs\": [20, 50, 150],\n",
        "        \"batch_size\": [128, 128, 128]\n",
        "    },\n",
        "\n",
        "    \"SA Northern Cape\": {\n",
        "        \"arquivo\": \"Timeseries_SA_northern_cape_2005_2016.csv\",\n",
        "        \"tipo\": \"tmy\",\n",
        "        \"targets\": [\"GSR\"],\n",
        "        \"ignorar\": [\"GSR\", \"DSR\"],\n",
        "        \"neurons\": [[100, 50]],\n",
        "        \"dropouts\": [[0, 0]],\n",
        "        \"epochs\": [20],\n",
        "        \"batch_size\": [512]\n",
        "    },\n",
        "\n",
        "    \"CAR Vakaga\": {\n",
        "        \"arquivo\": \"Timeseries_CAR_vakaga_2005_2016.csv\",\n",
        "        \"tipo\": \"tmy\",\n",
        "        \"targets\": [\"GSR\"],\n",
        "        \"ignorar\": [\"GSR\", \"DSR\"],\n",
        "        \"neurons\": [[200, 200, 100]],\n",
        "        \"dropouts\": [[0, 0, 0]],\n",
        "        \"epochs\": [30],\n",
        "        \"batch_size\": [512]\n",
        "    },\n",
        "\n",
        "    \"Egypt Mut\": {\n",
        "        \"arquivo\": \"Timeseries_egypt_mut_2005_2016.csv\",\n",
        "        \"tipo\": \"tmy\",\n",
        "        \"targets\": [\"GSR\"],\n",
        "        \"ignorar\": [\"GSR\", \"DSR\"],\n",
        "        \"neurons\": [[200, 200, 100]],\n",
        "        \"dropouts\": [[0, 0, 0]],\n",
        "        \"epochs\": [7],\n",
        "        \"batch_size\": [128]\n",
        "    },\n",
        "\n",
        "    \"Algeria Tamanrasset\": {\n",
        "        \"arquivo\": \"Timeseries_tamaransset_2005_2016.csv\",\n",
        "        \"tipo\": \"tmy\",\n",
        "        \"targets\": [\"GSR\"],\n",
        "        \"ignorar\": [\"GSR\", \"DSR\"],\n",
        "        \"neurons\": [[100, 100, 50]],\n",
        "        \"dropouts\": [[0, 0, 0]],\n",
        "        \"epochs\": [100],\n",
        "        \"batch_size\": [512]\n",
        "    },\n",
        "\n",
        "    \"Nigeria Borno\": {\n",
        "        \"arquivo\": \"Timeseries_nigeria_borno_2005_2016.csv\",\n",
        "        \"tipo\": \"tmy\",\n",
        "        \"targets\": [\"DSR\"],\n",
        "        \"ignorar\": [\"GSR\", \"DSR\"],\n",
        "        \"neurons\": [[100, 50]],\n",
        "        \"dropouts\": [[0, 0]],\n",
        "        \"epochs\": [50],\n",
        "        \"batch_size\": [512]\n",
        "    },\n",
        "\n",
        "    \"Nigeria Abuja\": {\n",
        "        \"arquivo\": \"SARAH_nigeria_abuja.csv\",\n",
        "        \"tipo\": \"sarah\",\n",
        "        \"targets\": [\"DNI\"],\n",
        "        \"ignorar\": [\"DNI\"],\n",
        "        \"neurons\": [[200, 200, 50]],\n",
        "        \"dropouts\": [[0, 0, 0]],\n",
        "        \"epochs\": [100],\n",
        "        \"batch_size\": [128]\n",
        "    },\n",
        "\n",
        "    \"Nigeria Akure\": {\n",
        "        \"arquivo\": \"SARAH_nigeria_akure.csv\",\n",
        "        \"tipo\": \"sarah\",\n",
        "        \"targets\": [\"DNI\"],\n",
        "        \"ignorar\": [\"DNI\"],\n",
        "        \"neurons\": [[200, 200, 100]],\n",
        "        \"dropouts\": [[0, 0, 0]],\n",
        "        \"epochs\": [100],\n",
        "        \"batch_size\": [128]\n",
        "    }\n",
        "}\n",
        "\n",
        "OTIMIZADORES = [\"adam\", \"sgd\"]\n",
        "\n",
        "LRS = [1e-3, 5e-4, 1e-4]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def carregar_wb(path):\n",
        "    df = pd.read_csv(path, sep=';')\n",
        "    df['time'] = pd.to_datetime(df['time'])\n",
        "\n",
        "    df['year'] = df['time'].dt.year\n",
        "    df['month'] = df['time'].dt.month\n",
        "    df['day'] = df['time'].dt.day\n",
        "    df['hour'] = df['time'].dt.hour\n",
        "    df['minute'] = df['time'].dt.minute\n",
        "\n",
        "    df.drop(columns=['time', 'comments'], inplace=True)\n",
        "\n",
        "    return df[\n",
        "        ['year','month','day','hour','minute','air_temperature',\n",
        "         'relative_humidity','wind_speed','wind_from_direction',\n",
        "         'wind_speed_calc','sensor_cleaning','precipitation',\n",
        "         'barometric_pressure','dhi_rsi','ghi_sil','ghi_pyr']\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "GX8qtY6gLrbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "1jQNbUqMVpb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def carregar_tmy(path):\n",
        "    df = pd.read_csv(\n",
        "        path, sep=',', skiprows=8, skipfooter=12, engine='python'\n",
        "    )\n",
        "\n",
        "    df['time'] = pd.to_datetime(df['time'], format=\"%Y%m%d:%H%M\")\n",
        "\n",
        "    df['year'] = df['time'].dt.year\n",
        "    df['month'] = df['time'].dt.month\n",
        "    df['day'] = df['time'].dt.day\n",
        "    df['hour'] = df['time'].dt.hour\n",
        "\n",
        "    df.drop(columns=['time','Int','Gr(i)'], inplace=True)\n",
        "\n",
        "    df = df.rename(columns={ 'Gb(i)': 'GSR', 'Gd(i)': 'DSR' })\n",
        "\n",
        "    return df[['year','month','day','hour','H_sun','T2m','WS10m','GSR','DSR']]\n"
      ],
      "metadata": {
        "id": "JwWVEktALwE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def carregar_sarah(path):\n",
        "  df = pd.read_csv(path, sep=';')\n",
        "\n",
        "  df['time'] = pd.to_datetime(df['time'], format=\"%d/%m/%Y\")\n",
        "\n",
        "  df['year'] = df['time'].dt.year\n",
        "  df['month'] = df['time'].dt.month\n",
        "  df['day'] = df['time'].dt.day\n",
        "\n",
        "  df.drop(columns=['time'], inplace=True)\n",
        "\n",
        "  return df[['year','month','day','SDU','DNI']]\n"
      ],
      "metadata": {
        "id": "h0BzeLM8TMTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def carregar_dataframe(cidade, config):\n",
        "    if config[\"tipo\"] == \"wb\":\n",
        "        return carregar_wb(config[\"arquivo\"])\n",
        "    elif config[\"tipo\"] == \"tmy\":\n",
        "        return carregar_tmy(config[\"arquivo\"])\n",
        "    elif config[\"tipo\"] == \"sarah\":\n",
        "      return carregar_sarah(config[\"arquivo\"])"
      ],
      "metadata": {
        "id": "ria2oqRsL0qU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "cNvNVuAtL9dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def criar_loaders(df, target, ignorar, batch_size):\n",
        "    X = df.drop(columns=ignorar).values\n",
        "    y = df[target].values.reshape(-1, 1)\n",
        "\n",
        "    # scaler_X = StandardScaler()\n",
        "    # scaler_y = StandardScaler()\n",
        "\n",
        "    # X = scaler_X.fit_transform(X)\n",
        "    # y = scaler_y.fit_transform(y)\n",
        "\n",
        "    n = len(df)\n",
        "    t = int(0.9 * n)\n",
        "\n",
        "    X_train = torch.tensor(X[:t], dtype=torch.float32)\n",
        "    y_train = torch.tensor(y[:t], dtype=torch.float32)\n",
        "    X_test  = torch.tensor(X[t:], dtype=torch.float32)\n",
        "    y_test  = torch.tensor(y[t:], dtype=torch.float32)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        TensorDataset(X_train, y_train),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        TensorDataset(X_test, y_test),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader, X_train.shape[1]#, scaler_y\n"
      ],
      "metadata": {
        "id": "RX-AOyKWL6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, num_features, num_outputs, neurons, dropouts):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        in_f = num_features\n",
        "\n",
        "        for out_f, drop in zip(neurons, dropouts):\n",
        "            layers.append(nn.Linear(in_f, out_f))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(drop))\n",
        "            in_f = out_f\n",
        "\n",
        "        layers.append(nn.Linear(in_f, num_outputs))\n",
        "\n",
        "        self.rede = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.rede(x)\n"
      ],
      "metadata": {
        "id": "cvVvx7n1UzZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models_data = []\n",
        "\n",
        "for cidade, cfg in CIDADES.items():\n",
        "\n",
        "    df = carregar_dataframe(cidade, cfg)\n",
        "    df = df.dropna()\n",
        "\n",
        "    for idx, target in enumerate(cfg[\"targets\"]):\n",
        "\n",
        "        #train_loader, test_loader, n_features, scaler_y = criar_loaders(\n",
        "        train_loader, test_loader, n_features = criar_loaders(\n",
        "            df,\n",
        "            target,\n",
        "            cfg[\"ignorar\"],\n",
        "            cfg[\"batch_size\"][idx]\n",
        "        )\n",
        "\n",
        "        params = {\n",
        "            \"neurons\": cfg[\"neurons\"][idx],\n",
        "            \"dropouts\": cfg[\"dropouts\"][idx],\n",
        "            \"epochs\": cfg[\"epochs\"][idx]\n",
        "        }\n",
        "\n",
        "        models_data.append({\n",
        "            \"tipo\": cfg['tipo'],\n",
        "            \"cidade\": cidade,\n",
        "            \"target\": target,\n",
        "            \"qnt_features\": n_features,\n",
        "            \"train_loader\": train_loader,\n",
        "            \"test_loader\": test_loader,\n",
        "            \"params\": params\n",
        "            #\"scaler_y\": scaler_y\n",
        "        })\n"
      ],
      "metadata": {
        "id": "B6diZDeYMKGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "l12kW5w1wFfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"output\", exist_ok=True)\n",
        "\n",
        "for md in models_data:\n",
        "  for opt_name in OTIMIZADORES:\n",
        "      for lr in LRS:\n",
        "\n",
        "        model = Model(\n",
        "            num_features=md[\"qnt_features\"],\n",
        "            num_outputs=1,\n",
        "            neurons=md[\"params\"][\"neurons\"],\n",
        "            dropouts=md[\"params\"][\"dropouts\"]\n",
        "        )\n",
        "\n",
        "        criterio = nn.MSELoss()\n",
        "\n",
        "        if opt_name == \"adam\":\n",
        "          opt = torch.optim.Adam(\n",
        "              model.parameters(),\n",
        "              lr=lr\n",
        "          )\n",
        "        elif opt_name == \"sgd\":\n",
        "            opt = torch.optim.SGD(\n",
        "                model.parameters(),\n",
        "                lr=lr,\n",
        "                momentum=0.9,\n",
        "                weight_decay=1e-4\n",
        "            )\n",
        "\n",
        "        best_loss = np.inf\n",
        "        best_state = None\n",
        "\n",
        "        print(f\"\\nTreinando {md['cidade']} - {md['target']} | opt={opt_name} | lr={lr}\")\n",
        "\n",
        "\n",
        "        epochs = md[\"params\"][\"epochs\"]\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            epoch_loss = 0\n",
        "            for Xb, yb in md[\"train_loader\"]:\n",
        "\n",
        "                pred = model(Xb)\n",
        "                loss = criterio(pred, yb)\n",
        "\n",
        "                opt.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # para SGD apenas\n",
        "                opt.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            epoch_loss /= len(md[\"train_loader\"])\n",
        "            print(f\"Epoch {epoch+1}/{epochs} | Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "            if epoch_loss < best_loss:\n",
        "                best_loss = epoch_loss\n",
        "                best_state = model.state_dict()\n",
        "\n",
        "        torch.save(best_state, f\"output/{md['cidade']}_{md['target']}_{opt_name}_{lr}.pth\")\n"
      ],
      "metadata": {
        "id": "mYRQ-ul4U_V-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7ca3f92-c4d4-460b-e0f2-ddc5776f2aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treinando Nigeria Abuja - DNI | opt=adam | lr=0.001\n",
            "Epoch 1/100 | Loss: 8910.4555\n",
            "Epoch 2/100 | Loss: 8075.5114\n",
            "Epoch 3/100 | Loss: 7986.0753\n",
            "Epoch 4/100 | Loss: 7880.9653\n",
            "Epoch 5/100 | Loss: 8084.1685\n",
            "Epoch 6/100 | Loss: 7865.5479\n",
            "Epoch 7/100 | Loss: 7781.1363\n",
            "Epoch 8/100 | Loss: 7756.0543\n",
            "Epoch 9/100 | Loss: 7650.7710\n",
            "Epoch 10/100 | Loss: 7302.8310\n",
            "Epoch 11/100 | Loss: 7385.4054\n",
            "Epoch 12/100 | Loss: 6397.4898\n",
            "Epoch 13/100 | Loss: 5754.5896\n",
            "Epoch 14/100 | Loss: 6435.6766\n",
            "Epoch 15/100 | Loss: 4749.9973\n",
            "Epoch 16/100 | Loss: 3400.4294\n",
            "Epoch 17/100 | Loss: 4284.8230\n",
            "Epoch 18/100 | Loss: 4472.4180\n",
            "Epoch 19/100 | Loss: 3138.0044\n",
            "Epoch 20/100 | Loss: 2985.9199\n",
            "Epoch 21/100 | Loss: 5859.4262\n",
            "Epoch 22/100 | Loss: 2423.2697\n",
            "Epoch 23/100 | Loss: 2199.2530\n",
            "Epoch 24/100 | Loss: 2491.1724\n",
            "Epoch 25/100 | Loss: 2335.2871\n",
            "Epoch 26/100 | Loss: 2066.7455\n",
            "Epoch 27/100 | Loss: 3234.1003\n",
            "Epoch 28/100 | Loss: 2466.8543\n",
            "Epoch 29/100 | Loss: 2350.6411\n",
            "Epoch 30/100 | Loss: 2193.1961\n",
            "Epoch 31/100 | Loss: 3293.6690\n",
            "Epoch 32/100 | Loss: 3254.5639\n",
            "Epoch 33/100 | Loss: 2185.0050\n",
            "Epoch 34/100 | Loss: 2284.8860\n",
            "Epoch 35/100 | Loss: 2949.4707\n",
            "Epoch 36/100 | Loss: 2525.6635\n",
            "Epoch 37/100 | Loss: 1696.8360\n",
            "Epoch 38/100 | Loss: 1644.6097\n",
            "Epoch 39/100 | Loss: 2389.5473\n",
            "Epoch 40/100 | Loss: 2458.4296\n",
            "Epoch 41/100 | Loss: 2405.1916\n",
            "Epoch 42/100 | Loss: 1909.1019\n",
            "Epoch 43/100 | Loss: 2096.6546\n",
            "Epoch 44/100 | Loss: 1854.7114\n",
            "Epoch 45/100 | Loss: 1953.4793\n",
            "Epoch 46/100 | Loss: 2194.1925\n",
            "Epoch 47/100 | Loss: 2145.6380\n",
            "Epoch 48/100 | Loss: 1787.2593\n",
            "Epoch 49/100 | Loss: 2186.6649\n",
            "Epoch 50/100 | Loss: 1578.3891\n",
            "Epoch 51/100 | Loss: 1952.7804\n",
            "Epoch 52/100 | Loss: 2400.5273\n",
            "Epoch 53/100 | Loss: 1551.2880\n",
            "Epoch 54/100 | Loss: 1554.7290\n",
            "Epoch 55/100 | Loss: 1728.1686\n",
            "Epoch 56/100 | Loss: 1891.6465\n",
            "Epoch 57/100 | Loss: 1623.8698\n",
            "Epoch 58/100 | Loss: 2067.9440\n",
            "Epoch 59/100 | Loss: 1887.4614\n",
            "Epoch 60/100 | Loss: 1850.7993\n",
            "Epoch 61/100 | Loss: 1933.8760\n",
            "Epoch 62/100 | Loss: 1759.2494\n",
            "Epoch 63/100 | Loss: 1522.8577\n",
            "Epoch 64/100 | Loss: 1541.9192\n",
            "Epoch 65/100 | Loss: 1466.6314\n",
            "Epoch 66/100 | Loss: 1509.1226\n",
            "Epoch 67/100 | Loss: 1690.0569\n",
            "Epoch 68/100 | Loss: 1858.4941\n",
            "Epoch 69/100 | Loss: 1827.1081\n",
            "Epoch 70/100 | Loss: 1558.2944\n",
            "Epoch 71/100 | Loss: 1693.4560\n",
            "Epoch 72/100 | Loss: 1701.0417\n",
            "Epoch 73/100 | Loss: 1790.4130\n",
            "Epoch 74/100 | Loss: 1435.1836\n",
            "Epoch 75/100 | Loss: 1705.7969\n",
            "Epoch 76/100 | Loss: 1711.2814\n",
            "Epoch 77/100 | Loss: 1784.7499\n",
            "Epoch 78/100 | Loss: 1667.5269\n",
            "Epoch 79/100 | Loss: 1532.4460\n",
            "Epoch 80/100 | Loss: 1488.6474\n",
            "Epoch 81/100 | Loss: 1591.3873\n",
            "Epoch 82/100 | Loss: 1627.8686\n",
            "Epoch 83/100 | Loss: 1467.2800\n",
            "Epoch 84/100 | Loss: 1590.9245\n",
            "Epoch 85/100 | Loss: 1503.7526\n",
            "Epoch 86/100 | Loss: 1526.5015\n",
            "Epoch 87/100 | Loss: 1883.2311\n",
            "Epoch 88/100 | Loss: 1386.0631\n",
            "Epoch 89/100 | Loss: 1617.7861\n",
            "Epoch 90/100 | Loss: 1458.6628\n",
            "Epoch 91/100 | Loss: 1475.9573\n",
            "Epoch 92/100 | Loss: 1629.4767\n",
            "Epoch 93/100 | Loss: 1616.1544\n",
            "Epoch 94/100 | Loss: 1429.7478\n",
            "Epoch 95/100 | Loss: 1648.2551\n",
            "Epoch 96/100 | Loss: 1535.8574\n",
            "Epoch 97/100 | Loss: 1543.7729\n",
            "Epoch 98/100 | Loss: 1527.3667\n",
            "Epoch 99/100 | Loss: 1353.2893\n",
            "Epoch 100/100 | Loss: 1400.0452\n",
            "\n",
            "Treinando Nigeria Abuja - DNI | opt=adam | lr=0.0005\n",
            "Epoch 1/100 | Loss: 8573.6745\n",
            "Epoch 2/100 | Loss: 8014.5834\n",
            "Epoch 3/100 | Loss: 7913.1992\n",
            "Epoch 4/100 | Loss: 7942.7993\n",
            "Epoch 5/100 | Loss: 7886.9292\n",
            "Epoch 6/100 | Loss: 7892.8549\n",
            "Epoch 7/100 | Loss: 7852.5729\n",
            "Epoch 8/100 | Loss: 7689.3209\n",
            "Epoch 9/100 | Loss: 7954.5617\n",
            "Epoch 10/100 | Loss: 7743.8357\n",
            "Epoch 11/100 | Loss: 7684.9951\n",
            "Epoch 12/100 | Loss: 7687.5129\n",
            "Epoch 13/100 | Loss: 7655.6201\n",
            "Epoch 14/100 | Loss: 7550.4091\n",
            "Epoch 15/100 | Loss: 7587.4009\n",
            "Epoch 16/100 | Loss: 7386.9838\n",
            "Epoch 17/100 | Loss: 7138.3572\n",
            "Epoch 18/100 | Loss: 7080.9360\n",
            "Epoch 19/100 | Loss: 6623.3340\n",
            "Epoch 20/100 | Loss: 6453.9420\n",
            "Epoch 21/100 | Loss: 5583.2604\n",
            "Epoch 22/100 | Loss: 6683.7469\n",
            "Epoch 23/100 | Loss: 5019.6723\n",
            "Epoch 24/100 | Loss: 5381.0373\n",
            "Epoch 25/100 | Loss: 4166.4163\n",
            "Epoch 26/100 | Loss: 3399.6383\n",
            "Epoch 27/100 | Loss: 3759.7605\n",
            "Epoch 28/100 | Loss: 2891.4311\n",
            "Epoch 29/100 | Loss: 2746.5198\n",
            "Epoch 30/100 | Loss: 4075.5647\n",
            "Epoch 31/100 | Loss: 2976.1745\n",
            "Epoch 32/100 | Loss: 3473.2889\n",
            "Epoch 33/100 | Loss: 3013.8759\n",
            "Epoch 34/100 | Loss: 3369.5580\n",
            "Epoch 35/100 | Loss: 3100.9133\n",
            "Epoch 36/100 | Loss: 4696.3182\n",
            "Epoch 37/100 | Loss: 3074.9072\n",
            "Epoch 38/100 | Loss: 3951.3679\n",
            "Epoch 39/100 | Loss: 5263.3242\n",
            "Epoch 40/100 | Loss: 2614.7132\n",
            "Epoch 41/100 | Loss: 2353.7150\n",
            "Epoch 42/100 | Loss: 2732.1634\n",
            "Epoch 43/100 | Loss: 2344.0225\n",
            "Epoch 44/100 | Loss: 2357.1777\n",
            "Epoch 45/100 | Loss: 2134.4048\n",
            "Epoch 46/100 | Loss: 2371.6250\n",
            "Epoch 47/100 | Loss: 2084.8683\n",
            "Epoch 48/100 | Loss: 2132.7625\n",
            "Epoch 49/100 | Loss: 3173.1692\n",
            "Epoch 50/100 | Loss: 2500.1548\n",
            "Epoch 51/100 | Loss: 2528.2754\n",
            "Epoch 52/100 | Loss: 2027.5073\n",
            "Epoch 53/100 | Loss: 2327.4385\n",
            "Epoch 54/100 | Loss: 2057.1293\n",
            "Epoch 55/100 | Loss: 2210.3379\n",
            "Epoch 56/100 | Loss: 2404.6697\n",
            "Epoch 57/100 | Loss: 4663.1039\n",
            "Epoch 58/100 | Loss: 2174.6727\n",
            "Epoch 59/100 | Loss: 2557.8643\n",
            "Epoch 60/100 | Loss: 2706.0181\n",
            "Epoch 61/100 | Loss: 2826.9836\n",
            "Epoch 62/100 | Loss: 2377.1585\n",
            "Epoch 63/100 | Loss: 2353.5224\n",
            "Epoch 64/100 | Loss: 1893.8125\n",
            "Epoch 65/100 | Loss: 2851.1771\n",
            "Epoch 66/100 | Loss: 1812.6636\n",
            "Epoch 67/100 | Loss: 1928.2023\n",
            "Epoch 68/100 | Loss: 2129.8396\n",
            "Epoch 69/100 | Loss: 1622.2890\n",
            "Epoch 70/100 | Loss: 1657.7066\n",
            "Epoch 71/100 | Loss: 1911.1274\n",
            "Epoch 72/100 | Loss: 2054.7541\n",
            "Epoch 73/100 | Loss: 2945.9815\n",
            "Epoch 74/100 | Loss: 1795.6240\n",
            "Epoch 75/100 | Loss: 1771.0503\n",
            "Epoch 76/100 | Loss: 1715.5095\n",
            "Epoch 77/100 | Loss: 1919.0524\n",
            "Epoch 78/100 | Loss: 1973.7781\n",
            "Epoch 79/100 | Loss: 1675.9673\n",
            "Epoch 80/100 | Loss: 1954.2584\n",
            "Epoch 81/100 | Loss: 1773.0533\n",
            "Epoch 82/100 | Loss: 2738.5874\n",
            "Epoch 83/100 | Loss: 1941.0038\n",
            "Epoch 84/100 | Loss: 1851.7368\n",
            "Epoch 85/100 | Loss: 1709.1910\n",
            "Epoch 86/100 | Loss: 2705.5819\n",
            "Epoch 87/100 | Loss: 2029.6768\n",
            "Epoch 88/100 | Loss: 2543.3638\n",
            "Epoch 89/100 | Loss: 1861.1897\n",
            "Epoch 90/100 | Loss: 1853.6058\n",
            "Epoch 91/100 | Loss: 1564.6409\n",
            "Epoch 92/100 | Loss: 1865.7742\n",
            "Epoch 93/100 | Loss: 1978.4381\n",
            "Epoch 94/100 | Loss: 1980.3345\n",
            "Epoch 95/100 | Loss: 1848.5204\n",
            "Epoch 96/100 | Loss: 1589.8756\n",
            "Epoch 97/100 | Loss: 2576.3399\n",
            "Epoch 98/100 | Loss: 2293.6296\n",
            "Epoch 99/100 | Loss: 2064.2816\n",
            "Epoch 100/100 | Loss: 1677.9522\n",
            "\n",
            "Treinando Nigeria Abuja - DNI | opt=adam | lr=0.0001\n",
            "Epoch 1/100 | Loss: 12469.1330\n",
            "Epoch 2/100 | Loss: 7555.1826\n",
            "Epoch 3/100 | Loss: 7593.9380\n",
            "Epoch 4/100 | Loss: 7551.2366\n",
            "Epoch 5/100 | Loss: 7574.5180\n",
            "Epoch 6/100 | Loss: 7535.1054\n",
            "Epoch 7/100 | Loss: 7559.8555\n",
            "Epoch 8/100 | Loss: 7601.7277\n",
            "Epoch 9/100 | Loss: 7545.9326\n",
            "Epoch 10/100 | Loss: 7578.9710\n",
            "Epoch 11/100 | Loss: 7538.3656\n",
            "Epoch 12/100 | Loss: 7558.0700\n",
            "Epoch 13/100 | Loss: 7521.8888\n",
            "Epoch 14/100 | Loss: 7550.0249\n",
            "Epoch 15/100 | Loss: 7600.5848\n",
            "Epoch 16/100 | Loss: 7539.4162\n",
            "Epoch 17/100 | Loss: 7595.9236\n",
            "Epoch 18/100 | Loss: 7536.7455\n",
            "Epoch 19/100 | Loss: 7528.3460\n",
            "Epoch 20/100 | Loss: 7549.2752\n",
            "Epoch 21/100 | Loss: 7512.5639\n",
            "Epoch 22/100 | Loss: 7550.5282\n",
            "Epoch 23/100 | Loss: 7507.9445\n",
            "Epoch 24/100 | Loss: 7554.3741\n",
            "Epoch 25/100 | Loss: 7510.1973\n",
            "Epoch 26/100 | Loss: 7564.7725\n",
            "Epoch 27/100 | Loss: 7514.1014\n",
            "Epoch 28/100 | Loss: 7504.7259\n",
            "Epoch 29/100 | Loss: 7535.3574\n",
            "Epoch 30/100 | Loss: 7491.5193\n",
            "Epoch 31/100 | Loss: 7547.1810\n",
            "Epoch 32/100 | Loss: 7500.7483\n",
            "Epoch 33/100 | Loss: 7503.5101\n",
            "Epoch 34/100 | Loss: 7554.8934\n",
            "Epoch 35/100 | Loss: 7502.7326\n",
            "Epoch 36/100 | Loss: 7471.5592\n",
            "Epoch 37/100 | Loss: 7563.3288\n",
            "Epoch 38/100 | Loss: 7511.0233\n",
            "Epoch 39/100 | Loss: 7467.1880\n",
            "Epoch 40/100 | Loss: 7472.6635\n",
            "Epoch 41/100 | Loss: 7520.4244\n",
            "Epoch 42/100 | Loss: 7467.0638\n",
            "Epoch 43/100 | Loss: 7447.2838\n",
            "Epoch 44/100 | Loss: 7455.9899\n",
            "Epoch 45/100 | Loss: 7510.5363\n",
            "Epoch 46/100 | Loss: 7456.1303\n",
            "Epoch 47/100 | Loss: 7431.0289\n",
            "Epoch 48/100 | Loss: 7442.3162\n",
            "Epoch 49/100 | Loss: 7510.7567\n",
            "Epoch 50/100 | Loss: 7457.7703\n",
            "Epoch 51/100 | Loss: 7420.2437\n",
            "Epoch 52/100 | Loss: 7404.3570\n",
            "Epoch 53/100 | Loss: 7418.6510\n",
            "Epoch 54/100 | Loss: 7390.8804\n",
            "Epoch 55/100 | Loss: 7389.0473\n",
            "Epoch 56/100 | Loss: 7380.7125\n",
            "Epoch 57/100 | Loss: 7383.3377\n",
            "Epoch 58/100 | Loss: 7363.1062\n",
            "Epoch 59/100 | Loss: 7393.9900\n",
            "Epoch 60/100 | Loss: 7363.5159\n",
            "Epoch 61/100 | Loss: 7340.8390\n",
            "Epoch 62/100 | Loss: 7355.8633\n",
            "Epoch 63/100 | Loss: 7324.9781\n",
            "Epoch 64/100 | Loss: 7317.3326\n",
            "Epoch 65/100 | Loss: 7312.4964\n",
            "Epoch 66/100 | Loss: 7293.2412\n",
            "Epoch 67/100 | Loss: 7291.1757\n",
            "Epoch 68/100 | Loss: 7269.6792\n",
            "Epoch 69/100 | Loss: 7261.8916\n",
            "Epoch 70/100 | Loss: 7249.3725\n",
            "Epoch 71/100 | Loss: 7289.2956\n",
            "Epoch 72/100 | Loss: 7284.7556\n",
            "Epoch 73/100 | Loss: 7179.8549\n",
            "Epoch 74/100 | Loss: 7106.7208\n",
            "Epoch 75/100 | Loss: 7081.3237\n",
            "Epoch 76/100 | Loss: 7048.9769\n",
            "Epoch 77/100 | Loss: 7034.6615\n",
            "Epoch 78/100 | Loss: 6996.2713\n",
            "Epoch 79/100 | Loss: 7010.2319\n",
            "Epoch 80/100 | Loss: 6910.0962\n",
            "Epoch 81/100 | Loss: 6877.8508\n",
            "Epoch 82/100 | Loss: 6818.4274\n",
            "Epoch 83/100 | Loss: 6736.4962\n",
            "Epoch 84/100 | Loss: 6666.6989\n",
            "Epoch 85/100 | Loss: 6684.9378\n",
            "Epoch 86/100 | Loss: 6564.1093\n",
            "Epoch 87/100 | Loss: 6465.5326\n",
            "Epoch 88/100 | Loss: 6380.5852\n",
            "Epoch 89/100 | Loss: 6307.7882\n",
            "Epoch 90/100 | Loss: 6258.8463\n",
            "Epoch 91/100 | Loss: 6086.4120\n",
            "Epoch 92/100 | Loss: 5967.3305\n",
            "Epoch 93/100 | Loss: 5963.8183\n",
            "Epoch 94/100 | Loss: 5751.7985\n",
            "Epoch 95/100 | Loss: 5671.6257\n",
            "Epoch 96/100 | Loss: 5586.1707\n",
            "Epoch 97/100 | Loss: 5343.7714\n",
            "Epoch 98/100 | Loss: 5160.3664\n",
            "Epoch 99/100 | Loss: 5149.8429\n",
            "Epoch 100/100 | Loss: 4939.3041\n",
            "\n",
            "Treinando Nigeria Abuja - DNI | opt=sgd | lr=0.001\n",
            "Epoch 1/100 | Loss: 8486.5847\n",
            "Epoch 2/100 | Loss: 7855.9659\n",
            "Epoch 3/100 | Loss: 7724.7110\n",
            "Epoch 4/100 | Loss: 7830.1967\n",
            "Epoch 5/100 | Loss: 7711.3098\n",
            "Epoch 6/100 | Loss: 7812.3187\n",
            "Epoch 7/100 | Loss: 7695.2444\n",
            "Epoch 8/100 | Loss: 7799.1700\n",
            "Epoch 9/100 | Loss: 7684.4411\n",
            "Epoch 10/100 | Loss: 7776.2981\n",
            "Epoch 11/100 | Loss: 7668.5967\n",
            "Epoch 12/100 | Loss: 7736.2674\n",
            "Epoch 13/100 | Loss: 7665.2644\n",
            "Epoch 14/100 | Loss: 7727.5936\n",
            "Epoch 15/100 | Loss: 7659.5751\n",
            "Epoch 16/100 | Loss: 7720.8382\n",
            "Epoch 17/100 | Loss: 7655.7958\n",
            "Epoch 18/100 | Loss: 7715.5944\n",
            "Epoch 19/100 | Loss: 7652.5625\n",
            "Epoch 20/100 | Loss: 7710.7118\n",
            "Epoch 21/100 | Loss: 7648.6988\n",
            "Epoch 22/100 | Loss: 7706.2773\n",
            "Epoch 23/100 | Loss: 7646.5823\n",
            "Epoch 24/100 | Loss: 7703.2836\n",
            "Epoch 25/100 | Loss: 7644.7930\n",
            "Epoch 26/100 | Loss: 7700.7501\n",
            "Epoch 27/100 | Loss: 7643.2758\n",
            "Epoch 28/100 | Loss: 7698.3918\n",
            "Epoch 29/100 | Loss: 7641.7034\n",
            "Epoch 30/100 | Loss: 7696.5069\n",
            "Epoch 31/100 | Loss: 7640.5824\n",
            "Epoch 32/100 | Loss: 7694.9482\n",
            "Epoch 33/100 | Loss: 7639.5810\n",
            "Epoch 34/100 | Loss: 7693.5901\n",
            "Epoch 35/100 | Loss: 7638.6460\n",
            "Epoch 36/100 | Loss: 7692.0463\n",
            "Epoch 37/100 | Loss: 7637.1955\n",
            "Epoch 38/100 | Loss: 7690.6462\n",
            "Epoch 39/100 | Loss: 7636.2508\n",
            "Epoch 40/100 | Loss: 7689.5483\n",
            "Epoch 41/100 | Loss: 7635.2050\n",
            "Epoch 42/100 | Loss: 7688.4089\n",
            "Epoch 43/100 | Loss: 7634.0010\n",
            "Epoch 44/100 | Loss: 7674.4502\n",
            "Epoch 45/100 | Loss: 7601.4714\n",
            "Epoch 46/100 | Loss: 7717.1857\n",
            "Epoch 47/100 | Loss: 7717.2277\n",
            "Epoch 48/100 | Loss: 7683.4878\n",
            "Epoch 49/100 | Loss: 7715.8316\n",
            "Epoch 50/100 | Loss: 7681.9445\n",
            "Epoch 51/100 | Loss: 7713.9452\n",
            "Epoch 52/100 | Loss: 7680.6182\n",
            "Epoch 53/100 | Loss: 7712.3605\n",
            "Epoch 54/100 | Loss: 7679.4292\n",
            "Epoch 55/100 | Loss: 7710.9998\n",
            "Epoch 56/100 | Loss: 7678.2123\n",
            "Epoch 57/100 | Loss: 7709.8159\n",
            "Epoch 58/100 | Loss: 7676.6967\n",
            "Epoch 59/100 | Loss: 7709.0122\n",
            "Epoch 60/100 | Loss: 7674.9961\n",
            "Epoch 61/100 | Loss: 7707.4887\n",
            "Epoch 62/100 | Loss: 7661.6539\n",
            "Epoch 63/100 | Loss: 7674.1661\n",
            "Epoch 64/100 | Loss: 7553.9402\n",
            "Epoch 65/100 | Loss: 7666.3950\n",
            "Epoch 66/100 | Loss: 7550.9149\n",
            "Epoch 67/100 | Loss: 7661.6161\n",
            "Epoch 68/100 | Loss: 7548.2523\n",
            "Epoch 69/100 | Loss: 7656.7611\n",
            "Epoch 70/100 | Loss: 7547.4224\n",
            "Epoch 71/100 | Loss: 7647.1417\n",
            "Epoch 72/100 | Loss: 7561.8693\n",
            "Epoch 73/100 | Loss: 7635.0606\n",
            "Epoch 74/100 | Loss: 7583.9789\n",
            "Epoch 75/100 | Loss: 7636.6063\n",
            "Epoch 76/100 | Loss: 7584.7320\n",
            "Epoch 77/100 | Loss: 7638.2808\n",
            "Epoch 78/100 | Loss: 7585.0286\n",
            "Epoch 79/100 | Loss: 7639.5934\n",
            "Epoch 80/100 | Loss: 7584.7810\n",
            "Epoch 81/100 | Loss: 7640.7449\n",
            "Epoch 82/100 | Loss: 7582.9615\n",
            "Epoch 83/100 | Loss: 7641.5595\n",
            "Epoch 84/100 | Loss: 7583.5127\n",
            "Epoch 85/100 | Loss: 7650.5152\n",
            "Epoch 86/100 | Loss: 7588.6680\n",
            "Epoch 87/100 | Loss: 7651.3207\n",
            "Epoch 88/100 | Loss: 7584.6093\n",
            "Epoch 89/100 | Loss: 7606.3902\n",
            "Epoch 90/100 | Loss: 7607.5780\n",
            "Epoch 91/100 | Loss: 7634.2490\n",
            "Epoch 92/100 | Loss: 7570.7013\n",
            "Epoch 93/100 | Loss: 7624.9375\n",
            "Epoch 94/100 | Loss: 7565.2074\n",
            "Epoch 95/100 | Loss: 7619.1913\n",
            "Epoch 96/100 | Loss: 7560.3295\n",
            "Epoch 97/100 | Loss: 7614.2202\n",
            "Epoch 98/100 | Loss: 7555.9169\n",
            "Epoch 99/100 | Loss: 7610.0762\n",
            "Epoch 100/100 | Loss: 7552.0297\n",
            "\n",
            "Treinando Nigeria Abuja - DNI | opt=sgd | lr=0.0005\n",
            "Epoch 1/100 | Loss: 10559.0915\n",
            "Epoch 2/100 | Loss: 7753.2612\n",
            "Epoch 3/100 | Loss: 7639.6045\n",
            "Epoch 4/100 | Loss: 7697.3461\n",
            "Epoch 5/100 | Loss: 7687.2039\n",
            "Epoch 6/100 | Loss: 7692.6316\n",
            "Epoch 7/100 | Loss: 7683.6509\n",
            "Epoch 8/100 | Loss: 7687.9683\n",
            "Epoch 9/100 | Loss: 7680.2008\n",
            "Epoch 10/100 | Loss: 7683.4650\n",
            "Epoch 11/100 | Loss: 7676.8442\n",
            "Epoch 12/100 | Loss: 7679.1048\n",
            "Epoch 13/100 | Loss: 7673.5638\n",
            "Epoch 14/100 | Loss: 7674.8682\n",
            "Epoch 15/100 | Loss: 7670.3402\n",
            "Epoch 16/100 | Loss: 7670.7312\n",
            "Epoch 17/100 | Loss: 7667.1457\n",
            "Epoch 18/100 | Loss: 7666.6636\n",
            "Epoch 19/100 | Loss: 7663.9462\n",
            "Epoch 20/100 | Loss: 7662.6220\n",
            "Epoch 21/100 | Loss: 7660.6927\n",
            "Epoch 22/100 | Loss: 7658.5474\n",
            "Epoch 23/100 | Loss: 7657.3070\n",
            "Epoch 24/100 | Loss: 7654.3983\n",
            "Epoch 25/100 | Loss: 7653.7615\n",
            "Epoch 26/100 | Loss: 7650.1675\n",
            "Epoch 27/100 | Loss: 7650.0698\n",
            "Epoch 28/100 | Loss: 7645.7818\n",
            "Epoch 29/100 | Loss: 7646.1614\n",
            "Epoch 30/100 | Loss: 7641.4606\n",
            "Epoch 31/100 | Loss: 7642.3450\n",
            "Epoch 32/100 | Loss: 7637.2838\n",
            "Epoch 33/100 | Loss: 7638.8258\n",
            "Epoch 34/100 | Loss: 7633.3902\n",
            "Epoch 35/100 | Loss: 7636.3173\n",
            "Epoch 36/100 | Loss: 7634.9356\n",
            "Epoch 37/100 | Loss: 7607.1460\n",
            "Epoch 38/100 | Loss: 7627.5788\n",
            "Epoch 39/100 | Loss: 7602.4462\n",
            "Epoch 40/100 | Loss: 7624.7861\n",
            "Epoch 41/100 | Loss: 7600.6144\n",
            "Epoch 42/100 | Loss: 7622.3788\n",
            "Epoch 43/100 | Loss: 7598.7495\n",
            "Epoch 44/100 | Loss: 7626.5709\n",
            "Epoch 45/100 | Loss: 7602.9228\n",
            "Epoch 46/100 | Loss: 7623.2280\n",
            "Epoch 47/100 | Loss: 7599.6583\n",
            "Epoch 48/100 | Loss: 7619.8338\n",
            "Epoch 49/100 | Loss: 7596.4158\n",
            "Epoch 50/100 | Loss: 7616.7115\n",
            "Epoch 51/100 | Loss: 7593.3867\n",
            "Epoch 52/100 | Loss: 7613.7538\n",
            "Epoch 53/100 | Loss: 7590.4855\n",
            "Epoch 54/100 | Loss: 7610.9089\n",
            "Epoch 55/100 | Loss: 7587.6544\n",
            "Epoch 56/100 | Loss: 7608.1709\n",
            "Epoch 57/100 | Loss: 7584.9009\n",
            "Epoch 58/100 | Loss: 7605.4892\n",
            "Epoch 59/100 | Loss: 7582.1593\n",
            "Epoch 60/100 | Loss: 7602.7202\n",
            "Epoch 61/100 | Loss: 7579.2737\n",
            "Epoch 62/100 | Loss: 7597.9484\n",
            "Epoch 63/100 | Loss: 7575.3935\n",
            "Epoch 64/100 | Loss: 7595.6260\n",
            "Epoch 65/100 | Loss: 7573.1583\n",
            "Epoch 66/100 | Loss: 7593.3487\n",
            "Epoch 67/100 | Loss: 7570.7437\n",
            "Epoch 68/100 | Loss: 7591.0671\n",
            "Epoch 69/100 | Loss: 7568.2210\n",
            "Epoch 70/100 | Loss: 7588.7506\n",
            "Epoch 71/100 | Loss: 7565.5876\n",
            "Epoch 72/100 | Loss: 7586.3989\n",
            "Epoch 73/100 | Loss: 7562.8649\n",
            "Epoch 74/100 | Loss: 7584.0126\n",
            "Epoch 75/100 | Loss: 7560.0400\n",
            "Epoch 76/100 | Loss: 7581.5667\n",
            "Epoch 77/100 | Loss: 7557.0724\n",
            "Epoch 78/100 | Loss: 7579.1210\n",
            "Epoch 79/100 | Loss: 7554.0591\n",
            "Epoch 80/100 | Loss: 7576.6552\n",
            "Epoch 81/100 | Loss: 7550.9593\n",
            "Epoch 82/100 | Loss: 7574.1664\n",
            "Epoch 83/100 | Loss: 7547.7655\n",
            "Epoch 84/100 | Loss: 7571.6562\n",
            "Epoch 85/100 | Loss: 7544.4778\n",
            "Epoch 86/100 | Loss: 7569.1277\n",
            "Epoch 87/100 | Loss: 7541.0962\n",
            "Epoch 88/100 | Loss: 7566.5885\n",
            "Epoch 89/100 | Loss: 7537.6192\n",
            "Epoch 90/100 | Loss: 7564.0013\n",
            "Epoch 91/100 | Loss: 7536.5758\n",
            "Epoch 92/100 | Loss: 7560.2775\n",
            "Epoch 93/100 | Loss: 7524.7167\n",
            "Epoch 94/100 | Loss: 7548.5463\n",
            "Epoch 95/100 | Loss: 7546.1142\n",
            "Epoch 96/100 | Loss: 7549.2933\n",
            "Epoch 97/100 | Loss: 7544.8888\n",
            "Epoch 98/100 | Loss: 7548.0199\n",
            "Epoch 99/100 | Loss: 7543.1600\n",
            "Epoch 100/100 | Loss: 7546.5191\n",
            "\n",
            "Treinando Nigeria Abuja - DNI | opt=sgd | lr=0.0001\n",
            "Epoch 1/100 | Loss: 18277.3913\n",
            "Epoch 2/100 | Loss: 7674.2336\n",
            "Epoch 3/100 | Loss: 7515.2001\n",
            "Epoch 4/100 | Loss: 7523.2239\n",
            "Epoch 5/100 | Loss: 7510.9663\n",
            "Epoch 6/100 | Loss: 7522.9791\n",
            "Epoch 7/100 | Loss: 7510.7058\n",
            "Epoch 8/100 | Loss: 7522.7364\n",
            "Epoch 9/100 | Loss: 7510.4365\n",
            "Epoch 10/100 | Loss: 7522.4571\n",
            "Epoch 11/100 | Loss: 7510.1500\n",
            "Epoch 12/100 | Loss: 7522.1788\n",
            "Epoch 13/100 | Loss: 7509.8479\n",
            "Epoch 14/100 | Loss: 7521.8556\n",
            "Epoch 15/100 | Loss: 7509.5286\n",
            "Epoch 16/100 | Loss: 7521.5632\n",
            "Epoch 17/100 | Loss: 7509.2224\n",
            "Epoch 18/100 | Loss: 7521.2411\n",
            "Epoch 19/100 | Loss: 7508.8975\n",
            "Epoch 20/100 | Loss: 7520.9261\n",
            "Epoch 21/100 | Loss: 7508.5713\n",
            "Epoch 22/100 | Loss: 7520.5915\n",
            "Epoch 23/100 | Loss: 7508.2391\n",
            "Epoch 24/100 | Loss: 7520.2705\n",
            "Epoch 25/100 | Loss: 7507.9065\n",
            "Epoch 26/100 | Loss: 7519.9292\n",
            "Epoch 27/100 | Loss: 7507.5674\n",
            "Epoch 28/100 | Loss: 7519.6009\n",
            "Epoch 29/100 | Loss: 7507.2167\n",
            "Epoch 30/100 | Loss: 7519.2221\n",
            "Epoch 31/100 | Loss: 7506.8585\n",
            "Epoch 32/100 | Loss: 7518.8916\n",
            "Epoch 33/100 | Loss: 7506.5216\n",
            "Epoch 34/100 | Loss: 7518.5561\n",
            "Epoch 35/100 | Loss: 7506.1774\n",
            "Epoch 36/100 | Loss: 7518.2023\n",
            "Epoch 37/100 | Loss: 7505.8270\n",
            "Epoch 38/100 | Loss: 7517.8620\n",
            "Epoch 39/100 | Loss: 7505.4768\n",
            "Epoch 40/100 | Loss: 7517.5023\n",
            "Epoch 41/100 | Loss: 7505.1226\n",
            "Epoch 42/100 | Loss: 7517.1582\n",
            "Epoch 43/100 | Loss: 7504.7702\n",
            "Epoch 44/100 | Loss: 7516.7975\n",
            "Epoch 45/100 | Loss: 7504.4138\n",
            "Epoch 46/100 | Loss: 7516.4496\n",
            "Epoch 47/100 | Loss: 7504.0569\n",
            "Epoch 48/100 | Loss: 7516.0829\n",
            "Epoch 49/100 | Loss: 7503.6965\n",
            "Epoch 50/100 | Loss: 7515.7321\n",
            "Epoch 51/100 | Loss: 7503.3384\n",
            "Epoch 52/100 | Loss: 7515.3661\n",
            "Epoch 53/100 | Loss: 7502.9774\n",
            "Epoch 54/100 | Loss: 7515.0141\n",
            "Epoch 55/100 | Loss: 7502.6178\n",
            "Epoch 56/100 | Loss: 7514.6456\n",
            "Epoch 57/100 | Loss: 7502.2542\n",
            "Epoch 58/100 | Loss: 7514.2900\n",
            "Epoch 59/100 | Loss: 7501.8909\n",
            "Epoch 60/100 | Loss: 7513.9183\n",
            "Epoch 61/100 | Loss: 7501.5249\n",
            "Epoch 62/100 | Loss: 7513.5613\n",
            "Epoch 63/100 | Loss: 7501.1606\n",
            "Epoch 64/100 | Loss: 7513.1887\n",
            "Epoch 65/100 | Loss: 7500.7934\n",
            "Epoch 66/100 | Loss: 7512.8304\n",
            "Epoch 67/100 | Loss: 7500.4281\n",
            "Epoch 68/100 | Loss: 7512.4569\n",
            "Epoch 69/100 | Loss: 7500.0592\n",
            "Epoch 70/100 | Loss: 7512.0963\n",
            "Epoch 71/100 | Loss: 7499.6915\n",
            "Epoch 72/100 | Loss: 7511.7202\n",
            "Epoch 73/100 | Loss: 7499.3212\n",
            "Epoch 74/100 | Loss: 7511.3588\n",
            "Epoch 75/100 | Loss: 7498.9533\n",
            "Epoch 76/100 | Loss: 7510.9834\n",
            "Epoch 77/100 | Loss: 7498.5826\n",
            "Epoch 78/100 | Loss: 7510.6206\n",
            "Epoch 79/100 | Loss: 7498.2126\n",
            "Epoch 80/100 | Loss: 7510.2427\n",
            "Epoch 81/100 | Loss: 7497.8399\n",
            "Epoch 82/100 | Loss: 7509.8789\n",
            "Epoch 83/100 | Loss: 7497.4747\n",
            "Epoch 84/100 | Loss: 7509.5146\n",
            "Epoch 85/100 | Loss: 7497.0961\n",
            "Epoch 86/100 | Loss: 7509.1155\n",
            "Epoch 87/100 | Loss: 7496.7117\n",
            "Epoch 88/100 | Loss: 7508.7515\n",
            "Epoch 89/100 | Loss: 7496.3464\n",
            "Epoch 90/100 | Loss: 7508.3877\n",
            "Epoch 91/100 | Loss: 7495.9612\n",
            "Epoch 92/100 | Loss: 7507.9731\n",
            "Epoch 93/100 | Loss: 7495.5698\n",
            "Epoch 94/100 | Loss: 7507.6109\n",
            "Epoch 95/100 | Loss: 7495.2049\n",
            "Epoch 96/100 | Loss: 7507.2478\n",
            "Epoch 97/100 | Loss: 7494.8393\n",
            "Epoch 98/100 | Loss: 7506.8832\n",
            "Epoch 99/100 | Loss: 7494.4534\n",
            "Epoch 100/100 | Loss: 7506.4685\n",
            "\n",
            "Treinando Nigeria Akure - DNI | opt=adam | lr=0.001\n",
            "Epoch 1/100 | Loss: 6813.1839\n",
            "Epoch 2/100 | Loss: 6287.3435\n",
            "Epoch 3/100 | Loss: 6030.1530\n",
            "Epoch 4/100 | Loss: 6005.8014\n",
            "Epoch 5/100 | Loss: 6063.3434\n",
            "Epoch 6/100 | Loss: 5909.6318\n",
            "Epoch 7/100 | Loss: 6031.1423\n",
            "Epoch 8/100 | Loss: 5168.1667\n",
            "Epoch 9/100 | Loss: 5013.1556\n",
            "Epoch 10/100 | Loss: 3627.1310\n",
            "Epoch 11/100 | Loss: 3230.7409\n",
            "Epoch 12/100 | Loss: 2558.2575\n",
            "Epoch 13/100 | Loss: 2549.3271\n",
            "Epoch 14/100 | Loss: 2779.3814\n",
            "Epoch 15/100 | Loss: 2133.2071\n",
            "Epoch 16/100 | Loss: 1865.6102\n",
            "Epoch 17/100 | Loss: 2591.6802\n",
            "Epoch 18/100 | Loss: 2051.9894\n",
            "Epoch 19/100 | Loss: 1598.7687\n",
            "Epoch 20/100 | Loss: 1394.8258\n",
            "Epoch 21/100 | Loss: 1535.1094\n",
            "Epoch 22/100 | Loss: 2120.5560\n",
            "Epoch 23/100 | Loss: 1763.3515\n",
            "Epoch 24/100 | Loss: 1475.5978\n",
            "Epoch 25/100 | Loss: 1463.3754\n",
            "Epoch 26/100 | Loss: 1382.8266\n",
            "Epoch 27/100 | Loss: 1394.1023\n",
            "Epoch 28/100 | Loss: 1372.4053\n",
            "Epoch 29/100 | Loss: 1245.2825\n",
            "Epoch 30/100 | Loss: 1525.7905\n",
            "Epoch 31/100 | Loss: 1457.7164\n",
            "Epoch 32/100 | Loss: 1631.5886\n",
            "Epoch 33/100 | Loss: 1346.5904\n",
            "Epoch 34/100 | Loss: 1630.4849\n",
            "Epoch 35/100 | Loss: 1257.9993\n",
            "Epoch 36/100 | Loss: 1273.2145\n",
            "Epoch 37/100 | Loss: 1265.8491\n",
            "Epoch 38/100 | Loss: 1333.1930\n",
            "Epoch 39/100 | Loss: 1594.9296\n",
            "Epoch 40/100 | Loss: 1476.2856\n",
            "Epoch 41/100 | Loss: 1425.5859\n",
            "Epoch 42/100 | Loss: 1733.4116\n",
            "Epoch 43/100 | Loss: 1281.4804\n",
            "Epoch 44/100 | Loss: 1343.6053\n",
            "Epoch 45/100 | Loss: 1725.3126\n",
            "Epoch 46/100 | Loss: 1256.8897\n",
            "Epoch 47/100 | Loss: 1519.8722\n",
            "Epoch 48/100 | Loss: 1387.3802\n",
            "Epoch 49/100 | Loss: 1344.5747\n",
            "Epoch 50/100 | Loss: 1313.1693\n",
            "Epoch 51/100 | Loss: 1276.8732\n",
            "Epoch 52/100 | Loss: 1405.0769\n",
            "Epoch 53/100 | Loss: 1331.3907\n",
            "Epoch 54/100 | Loss: 1395.8549\n",
            "Epoch 55/100 | Loss: 1516.5822\n",
            "Epoch 56/100 | Loss: 1481.9888\n",
            "Epoch 57/100 | Loss: 1337.8314\n",
            "Epoch 58/100 | Loss: 1130.4903\n",
            "Epoch 59/100 | Loss: 1273.9073\n",
            "Epoch 60/100 | Loss: 1499.1597\n",
            "Epoch 61/100 | Loss: 1144.0815\n",
            "Epoch 62/100 | Loss: 1274.8627\n",
            "Epoch 63/100 | Loss: 1417.3833\n",
            "Epoch 64/100 | Loss: 1307.3186\n",
            "Epoch 65/100 | Loss: 1684.3537\n",
            "Epoch 66/100 | Loss: 1418.1352\n",
            "Epoch 67/100 | Loss: 1386.1302\n",
            "Epoch 68/100 | Loss: 1606.2834\n",
            "Epoch 69/100 | Loss: 1439.4709\n",
            "Epoch 70/100 | Loss: 1276.7709\n",
            "Epoch 71/100 | Loss: 1214.1969\n",
            "Epoch 72/100 | Loss: 1325.7523\n",
            "Epoch 73/100 | Loss: 1219.0684\n",
            "Epoch 74/100 | Loss: 1575.6763\n",
            "Epoch 75/100 | Loss: 1564.2360\n",
            "Epoch 76/100 | Loss: 1224.9244\n",
            "Epoch 77/100 | Loss: 1259.7801\n",
            "Epoch 78/100 | Loss: 1130.9126\n",
            "Epoch 79/100 | Loss: 1316.5781\n",
            "Epoch 80/100 | Loss: 1330.7979\n",
            "Epoch 81/100 | Loss: 1175.3767\n",
            "Epoch 82/100 | Loss: 1268.0906\n",
            "Epoch 83/100 | Loss: 1634.1861\n",
            "Epoch 84/100 | Loss: 1085.9580\n",
            "Epoch 85/100 | Loss: 1321.2815\n",
            "Epoch 86/100 | Loss: 1167.2152\n",
            "Epoch 87/100 | Loss: 1170.0960\n",
            "Epoch 88/100 | Loss: 1293.0603\n",
            "Epoch 89/100 | Loss: 1154.2142\n",
            "Epoch 90/100 | Loss: 1069.2014\n",
            "Epoch 91/100 | Loss: 1097.1121\n",
            "Epoch 92/100 | Loss: 1060.0676\n",
            "Epoch 93/100 | Loss: 1091.4253\n",
            "Epoch 94/100 | Loss: 1093.5439\n",
            "Epoch 95/100 | Loss: 1488.1883\n",
            "Epoch 96/100 | Loss: 1131.9909\n",
            "Epoch 97/100 | Loss: 1126.6329\n",
            "Epoch 98/100 | Loss: 1349.5914\n",
            "Epoch 99/100 | Loss: 1412.7143\n",
            "Epoch 100/100 | Loss: 1213.9728\n",
            "\n",
            "Treinando Nigeria Akure - DNI | opt=adam | lr=0.0005\n",
            "Epoch 1/100 | Loss: 6653.9672\n",
            "Epoch 2/100 | Loss: 6066.2077\n",
            "Epoch 3/100 | Loss: 5984.5799\n",
            "Epoch 4/100 | Loss: 5904.7218\n",
            "Epoch 5/100 | Loss: 5916.2341\n",
            "Epoch 6/100 | Loss: 5796.0799\n",
            "Epoch 7/100 | Loss: 5718.9059\n",
            "Epoch 8/100 | Loss: 5574.0437\n",
            "Epoch 9/100 | Loss: 5498.0287\n",
            "Epoch 10/100 | Loss: 4909.8000\n",
            "Epoch 11/100 | Loss: 4766.8623\n",
            "Epoch 12/100 | Loss: 4440.2011\n",
            "Epoch 13/100 | Loss: 3051.2694\n",
            "Epoch 14/100 | Loss: 2503.5528\n",
            "Epoch 15/100 | Loss: 2352.2654\n",
            "Epoch 16/100 | Loss: 2193.3644\n",
            "Epoch 17/100 | Loss: 2008.4463\n",
            "Epoch 18/100 | Loss: 2451.9986\n",
            "Epoch 19/100 | Loss: 1763.1236\n",
            "Epoch 20/100 | Loss: 1807.8479\n",
            "Epoch 21/100 | Loss: 1631.6583\n",
            "Epoch 22/100 | Loss: 1958.2039\n",
            "Epoch 23/100 | Loss: 2227.1070\n",
            "Epoch 24/100 | Loss: 1655.9250\n",
            "Epoch 25/100 | Loss: 2035.8470\n",
            "Epoch 26/100 | Loss: 1793.9080\n",
            "Epoch 27/100 | Loss: 1808.2892\n",
            "Epoch 28/100 | Loss: 1756.8417\n",
            "Epoch 29/100 | Loss: 1706.3131\n",
            "Epoch 30/100 | Loss: 2081.6308\n",
            "Epoch 31/100 | Loss: 2009.4089\n",
            "Epoch 32/100 | Loss: 1744.8540\n",
            "Epoch 33/100 | Loss: 1856.4140\n",
            "Epoch 34/100 | Loss: 1707.3960\n",
            "Epoch 35/100 | Loss: 1818.1819\n",
            "Epoch 36/100 | Loss: 2252.8620\n",
            "Epoch 37/100 | Loss: 1798.5310\n",
            "Epoch 38/100 | Loss: 1552.6988\n",
            "Epoch 39/100 | Loss: 1640.5031\n",
            "Epoch 40/100 | Loss: 1618.4441\n",
            "Epoch 41/100 | Loss: 1732.4780\n",
            "Epoch 42/100 | Loss: 1593.2359\n",
            "Epoch 43/100 | Loss: 1510.0590\n",
            "Epoch 44/100 | Loss: 1441.4491\n",
            "Epoch 45/100 | Loss: 1430.6647\n",
            "Epoch 46/100 | Loss: 1573.2546\n",
            "Epoch 47/100 | Loss: 1644.3614\n",
            "Epoch 48/100 | Loss: 1514.9108\n",
            "Epoch 49/100 | Loss: 1648.8370\n",
            "Epoch 50/100 | Loss: 1556.2180\n",
            "Epoch 51/100 | Loss: 1471.8906\n",
            "Epoch 52/100 | Loss: 1643.7846\n",
            "Epoch 53/100 | Loss: 1292.0868\n",
            "Epoch 54/100 | Loss: 1526.6498\n",
            "Epoch 55/100 | Loss: 1575.9355\n",
            "Epoch 56/100 | Loss: 1216.3828\n",
            "Epoch 57/100 | Loss: 1565.3995\n",
            "Epoch 58/100 | Loss: 1564.6324\n",
            "Epoch 59/100 | Loss: 1687.6022\n",
            "Epoch 60/100 | Loss: 2197.9166\n",
            "Epoch 61/100 | Loss: 1349.9542\n",
            "Epoch 62/100 | Loss: 1669.0992\n",
            "Epoch 63/100 | Loss: 1541.1007\n",
            "Epoch 64/100 | Loss: 1462.6662\n",
            "Epoch 65/100 | Loss: 2107.9075\n",
            "Epoch 66/100 | Loss: 1718.6163\n",
            "Epoch 67/100 | Loss: 1569.3032\n",
            "Epoch 68/100 | Loss: 1438.8686\n",
            "Epoch 69/100 | Loss: 1707.7723\n",
            "Epoch 70/100 | Loss: 1278.1372\n",
            "Epoch 71/100 | Loss: 1619.0432\n",
            "Epoch 72/100 | Loss: 1290.7406\n",
            "Epoch 73/100 | Loss: 1534.2628\n",
            "Epoch 74/100 | Loss: 1410.7349\n",
            "Epoch 75/100 | Loss: 1532.5416\n",
            "Epoch 76/100 | Loss: 1613.7867\n",
            "Epoch 77/100 | Loss: 1824.3575\n",
            "Epoch 78/100 | Loss: 1426.2943\n",
            "Epoch 79/100 | Loss: 1360.1197\n",
            "Epoch 80/100 | Loss: 1288.3253\n",
            "Epoch 81/100 | Loss: 1541.3316\n",
            "Epoch 82/100 | Loss: 1150.6736\n",
            "Epoch 83/100 | Loss: 1283.4226\n",
            "Epoch 84/100 | Loss: 1456.0733\n",
            "Epoch 85/100 | Loss: 1422.2017\n",
            "Epoch 86/100 | Loss: 1377.0296\n",
            "Epoch 87/100 | Loss: 1409.6744\n",
            "Epoch 88/100 | Loss: 1283.1283\n",
            "Epoch 89/100 | Loss: 1625.3443\n",
            "Epoch 90/100 | Loss: 1242.2245\n",
            "Epoch 91/100 | Loss: 1241.0160\n",
            "Epoch 92/100 | Loss: 1979.2912\n",
            "Epoch 93/100 | Loss: 1131.2827\n",
            "Epoch 94/100 | Loss: 1168.9348\n",
            "Epoch 95/100 | Loss: 1172.9242\n",
            "Epoch 96/100 | Loss: 1655.4789\n",
            "Epoch 97/100 | Loss: 1437.2292\n",
            "Epoch 98/100 | Loss: 1173.7087\n",
            "Epoch 99/100 | Loss: 1338.7834\n",
            "Epoch 100/100 | Loss: 1222.2222\n",
            "\n",
            "Treinando Nigeria Akure - DNI | opt=adam | lr=0.0001\n",
            "Epoch 1/100 | Loss: 6841.8222\n",
            "Epoch 2/100 | Loss: 5906.4133\n",
            "Epoch 3/100 | Loss: 5870.6060\n",
            "Epoch 4/100 | Loss: 5868.1191\n",
            "Epoch 5/100 | Loss: 5862.4993\n",
            "Epoch 6/100 | Loss: 5859.8698\n",
            "Epoch 7/100 | Loss: 5848.8943\n",
            "Epoch 8/100 | Loss: 5834.2232\n",
            "Epoch 9/100 | Loss: 5801.0108\n",
            "Epoch 10/100 | Loss: 5793.0685\n",
            "Epoch 11/100 | Loss: 5734.0437\n",
            "Epoch 12/100 | Loss: 5734.7596\n",
            "Epoch 13/100 | Loss: 5718.9464\n",
            "Epoch 14/100 | Loss: 5723.4897\n",
            "Epoch 15/100 | Loss: 5730.3030\n",
            "Epoch 16/100 | Loss: 5616.5894\n",
            "Epoch 17/100 | Loss: 5626.4274\n",
            "Epoch 18/100 | Loss: 5615.6090\n",
            "Epoch 19/100 | Loss: 5545.7763\n",
            "Epoch 20/100 | Loss: 5473.3825\n",
            "Epoch 21/100 | Loss: 5429.1379\n",
            "Epoch 22/100 | Loss: 5350.3284\n",
            "Epoch 23/100 | Loss: 5342.2415\n",
            "Epoch 24/100 | Loss: 5263.9949\n",
            "Epoch 25/100 | Loss: 5110.9197\n",
            "Epoch 26/100 | Loss: 5072.9452\n",
            "Epoch 27/100 | Loss: 4907.2730\n",
            "Epoch 28/100 | Loss: 4798.5776\n",
            "Epoch 29/100 | Loss: 4691.6133\n",
            "Epoch 30/100 | Loss: 4607.3942\n",
            "Epoch 31/100 | Loss: 4428.1806\n",
            "Epoch 32/100 | Loss: 4456.0294\n",
            "Epoch 33/100 | Loss: 4175.1130\n",
            "Epoch 34/100 | Loss: 4229.2674\n",
            "Epoch 35/100 | Loss: 3796.6684\n",
            "Epoch 36/100 | Loss: 3742.7133\n",
            "Epoch 37/100 | Loss: 3351.2610\n",
            "Epoch 38/100 | Loss: 3331.1817\n",
            "Epoch 39/100 | Loss: 3032.8805\n",
            "Epoch 40/100 | Loss: 2681.5876\n",
            "Epoch 41/100 | Loss: 2980.6525\n",
            "Epoch 42/100 | Loss: 2687.3909\n",
            "Epoch 43/100 | Loss: 2722.3815\n",
            "Epoch 44/100 | Loss: 2426.0613\n",
            "Epoch 45/100 | Loss: 2436.9708\n",
            "Epoch 46/100 | Loss: 1975.1478\n",
            "Epoch 47/100 | Loss: 2668.0129\n",
            "Epoch 48/100 | Loss: 1648.7107\n",
            "Epoch 49/100 | Loss: 1659.2751\n",
            "Epoch 50/100 | Loss: 1486.1439\n",
            "Epoch 51/100 | Loss: 1625.6988\n",
            "Epoch 52/100 | Loss: 1669.7465\n",
            "Epoch 53/100 | Loss: 1595.3951\n",
            "Epoch 54/100 | Loss: 1543.7712\n",
            "Epoch 55/100 | Loss: 1863.9142\n",
            "Epoch 56/100 | Loss: 2131.2107\n",
            "Epoch 57/100 | Loss: 1718.4621\n",
            "Epoch 58/100 | Loss: 2698.6837\n",
            "Epoch 59/100 | Loss: 2931.6361\n",
            "Epoch 60/100 | Loss: 1820.6384\n",
            "Epoch 61/100 | Loss: 2003.9951\n",
            "Epoch 62/100 | Loss: 1484.3255\n",
            "Epoch 63/100 | Loss: 1984.4326\n",
            "Epoch 64/100 | Loss: 1420.5032\n",
            "Epoch 65/100 | Loss: 1369.1941\n",
            "Epoch 66/100 | Loss: 1666.5731\n",
            "Epoch 67/100 | Loss: 1468.5853\n",
            "Epoch 68/100 | Loss: 2770.4815\n",
            "Epoch 69/100 | Loss: 2510.9681\n",
            "Epoch 70/100 | Loss: 1535.9043\n",
            "Epoch 71/100 | Loss: 1525.9869\n",
            "Epoch 72/100 | Loss: 1334.0912\n",
            "Epoch 73/100 | Loss: 1622.1840\n",
            "Epoch 74/100 | Loss: 1258.3086\n",
            "Epoch 75/100 | Loss: 1289.1359\n",
            "Epoch 76/100 | Loss: 1703.3506\n",
            "Epoch 77/100 | Loss: 1488.9604\n",
            "Epoch 78/100 | Loss: 2088.7373\n",
            "Epoch 79/100 | Loss: 1700.3801\n",
            "Epoch 80/100 | Loss: 1340.8149\n",
            "Epoch 81/100 | Loss: 1449.2568\n",
            "Epoch 82/100 | Loss: 1738.9531\n",
            "Epoch 83/100 | Loss: 1706.9272\n",
            "Epoch 84/100 | Loss: 1332.3224\n",
            "Epoch 85/100 | Loss: 1278.1547\n",
            "Epoch 86/100 | Loss: 1721.2535\n",
            "Epoch 87/100 | Loss: 1658.2653\n",
            "Epoch 88/100 | Loss: 1463.7044\n",
            "Epoch 89/100 | Loss: 1292.5757\n",
            "Epoch 90/100 | Loss: 1717.0112\n",
            "Epoch 91/100 | Loss: 1743.4804\n",
            "Epoch 92/100 | Loss: 1669.7292\n",
            "Epoch 93/100 | Loss: 1336.1685\n",
            "Epoch 94/100 | Loss: 1835.0263\n",
            "Epoch 95/100 | Loss: 1770.1698\n",
            "Epoch 96/100 | Loss: 1586.2532\n",
            "Epoch 97/100 | Loss: 1546.4824\n",
            "Epoch 98/100 | Loss: 1601.8666\n",
            "Epoch 99/100 | Loss: 1466.0993\n",
            "Epoch 100/100 | Loss: 1404.6158\n",
            "\n",
            "Treinando Nigeria Akure - DNI | opt=sgd | lr=0.001\n",
            "Epoch 1/100 | Loss: 7769.7402\n",
            "Epoch 2/100 | Loss: 5940.4971\n",
            "Epoch 3/100 | Loss: 5998.1140\n",
            "Epoch 4/100 | Loss: 5935.3815\n",
            "Epoch 5/100 | Loss: 5956.7094\n",
            "Epoch 6/100 | Loss: 5901.3261\n",
            "Epoch 7/100 | Loss: 5971.4089\n",
            "Epoch 8/100 | Loss: 5918.3623\n",
            "Epoch 9/100 | Loss: 5968.2621\n",
            "Epoch 10/100 | Loss: 5912.3947\n",
            "Epoch 11/100 | Loss: 5868.5757\n",
            "Epoch 12/100 | Loss: 5908.0132\n",
            "Epoch 13/100 | Loss: 5864.9957\n",
            "Epoch 14/100 | Loss: 5902.7063\n",
            "Epoch 15/100 | Loss: 5860.0408\n",
            "Epoch 16/100 | Loss: 5896.6874\n",
            "Epoch 17/100 | Loss: 5855.3521\n",
            "Epoch 18/100 | Loss: 5892.3952\n",
            "Epoch 19/100 | Loss: 5941.3772\n",
            "Epoch 20/100 | Loss: 5881.9371\n",
            "Epoch 21/100 | Loss: 5932.0380\n",
            "Epoch 22/100 | Loss: 5878.4946\n",
            "Epoch 23/100 | Loss: 5930.2039\n",
            "Epoch 24/100 | Loss: 5871.8439\n",
            "Epoch 25/100 | Loss: 5923.2767\n",
            "Epoch 26/100 | Loss: 5893.2263\n",
            "Epoch 27/100 | Loss: 5829.6692\n",
            "Epoch 28/100 | Loss: 5872.4640\n",
            "Epoch 29/100 | Loss: 5827.6066\n",
            "Epoch 30/100 | Loss: 5847.0696\n",
            "Epoch 31/100 | Loss: 5825.1388\n",
            "Epoch 32/100 | Loss: 5904.2898\n",
            "Epoch 33/100 | Loss: 5864.3707\n",
            "Epoch 34/100 | Loss: 5841.8933\n",
            "Epoch 35/100 | Loss: 5811.0320\n",
            "Epoch 36/100 | Loss: 5882.9377\n",
            "Epoch 37/100 | Loss: 5837.2159\n",
            "Epoch 38/100 | Loss: 5880.5152\n",
            "Epoch 39/100 | Loss: 5871.9457\n",
            "Epoch 40/100 | Loss: 5771.5419\n",
            "Epoch 41/100 | Loss: 5796.0851\n",
            "Epoch 42/100 | Loss: 5740.0292\n",
            "Epoch 43/100 | Loss: 5800.7493\n",
            "Epoch 44/100 | Loss: 5733.0314\n",
            "Epoch 45/100 | Loss: 5783.7510\n",
            "Epoch 46/100 | Loss: 5745.7888\n",
            "Epoch 47/100 | Loss: 5790.3691\n",
            "Epoch 48/100 | Loss: 5743.8599\n",
            "Epoch 49/100 | Loss: 5810.2334\n",
            "Epoch 50/100 | Loss: 5730.8414\n",
            "Epoch 51/100 | Loss: 5701.4326\n",
            "Epoch 52/100 | Loss: 5702.2405\n",
            "Epoch 53/100 | Loss: 5691.4707\n",
            "Epoch 54/100 | Loss: 5707.0204\n",
            "Epoch 55/100 | Loss: 5699.8862\n",
            "Epoch 56/100 | Loss: 5669.0072\n",
            "Epoch 57/100 | Loss: 5687.5499\n",
            "Epoch 58/100 | Loss: 5662.6106\n",
            "Epoch 59/100 | Loss: 5685.9973\n",
            "Epoch 60/100 | Loss: 5740.7074\n",
            "Epoch 61/100 | Loss: 5617.5675\n",
            "Epoch 62/100 | Loss: 5576.8159\n",
            "Epoch 63/100 | Loss: 5646.5715\n",
            "Epoch 64/100 | Loss: 5599.2010\n",
            "Epoch 65/100 | Loss: 5600.2555\n",
            "Epoch 66/100 | Loss: 5581.5302\n",
            "Epoch 67/100 | Loss: 5488.8808\n",
            "Epoch 68/100 | Loss: 5688.1996\n",
            "Epoch 69/100 | Loss: 5614.6737\n",
            "Epoch 70/100 | Loss: 5779.0051\n",
            "Epoch 71/100 | Loss: 5439.4258\n",
            "Epoch 72/100 | Loss: 5770.8541\n",
            "Epoch 73/100 | Loss: 5772.0665\n",
            "Epoch 74/100 | Loss: 5585.4478\n",
            "Epoch 75/100 | Loss: 5469.7225\n",
            "Epoch 76/100 | Loss: 5406.6811\n",
            "Epoch 77/100 | Loss: 5440.4525\n",
            "Epoch 78/100 | Loss: 5292.1524\n",
            "Epoch 79/100 | Loss: 5370.9689\n",
            "Epoch 80/100 | Loss: 5485.2375\n",
            "Epoch 81/100 | Loss: 5218.6555\n",
            "Epoch 82/100 | Loss: 5476.8462\n",
            "Epoch 83/100 | Loss: 5192.1826\n",
            "Epoch 84/100 | Loss: 5462.6369\n",
            "Epoch 85/100 | Loss: 5172.9730\n",
            "Epoch 86/100 | Loss: 5332.2600\n",
            "Epoch 87/100 | Loss: 5278.6050\n",
            "Epoch 88/100 | Loss: 5085.9861\n",
            "Epoch 89/100 | Loss: 5228.7752\n",
            "Epoch 90/100 | Loss: 5045.3683\n",
            "Epoch 91/100 | Loss: 5210.5394\n",
            "Epoch 92/100 | Loss: 5025.9256\n",
            "Epoch 93/100 | Loss: 5194.5485\n",
            "Epoch 94/100 | Loss: 5003.6669\n",
            "Epoch 95/100 | Loss: 5176.8686\n",
            "Epoch 96/100 | Loss: 4980.1754\n",
            "Epoch 97/100 | Loss: 5156.4944\n",
            "Epoch 98/100 | Loss: 4960.6303\n",
            "Epoch 99/100 | Loss: 5402.7969\n",
            "Epoch 100/100 | Loss: 5068.4143\n",
            "\n",
            "Treinando Nigeria Akure - DNI | opt=sgd | lr=0.0005\n",
            "Epoch 1/100 | Loss: 6949.3882\n",
            "Epoch 2/100 | Loss: 5841.6058\n",
            "Epoch 3/100 | Loss: 5848.4332\n",
            "Epoch 4/100 | Loss: 5838.6899\n",
            "Epoch 5/100 | Loss: 5845.0292\n",
            "Epoch 6/100 | Loss: 5835.2402\n",
            "Epoch 7/100 | Loss: 5841.3926\n",
            "Epoch 8/100 | Loss: 5831.6637\n",
            "Epoch 9/100 | Loss: 5837.7579\n",
            "Epoch 10/100 | Loss: 5827.8961\n",
            "Epoch 11/100 | Loss: 5834.1109\n",
            "Epoch 12/100 | Loss: 5824.6400\n",
            "Epoch 13/100 | Loss: 5830.6956\n",
            "Epoch 14/100 | Loss: 5821.2507\n",
            "Epoch 15/100 | Loss: 5827.2573\n",
            "Epoch 16/100 | Loss: 5817.8764\n",
            "Epoch 17/100 | Loss: 5823.8495\n",
            "Epoch 18/100 | Loss: 5814.5292\n",
            "Epoch 19/100 | Loss: 5820.4656\n",
            "Epoch 20/100 | Loss: 5811.2008\n",
            "Epoch 21/100 | Loss: 5817.0921\n",
            "Epoch 22/100 | Loss: 5807.8827\n",
            "Epoch 23/100 | Loss: 5813.7117\n",
            "Epoch 24/100 | Loss: 5804.5667\n",
            "Epoch 25/100 | Loss: 5810.3035\n",
            "Epoch 26/100 | Loss: 5801.2502\n",
            "Epoch 27/100 | Loss: 5806.8383\n",
            "Epoch 28/100 | Loss: 5798.0691\n",
            "Epoch 29/100 | Loss: 5803.2959\n",
            "Epoch 30/100 | Loss: 5794.8827\n",
            "Epoch 31/100 | Loss: 5800.9390\n",
            "Epoch 32/100 | Loss: 5814.7362\n",
            "Epoch 33/100 | Loss: 5810.3316\n",
            "Epoch 34/100 | Loss: 5810.2378\n",
            "Epoch 35/100 | Loss: 5807.2403\n",
            "Epoch 36/100 | Loss: 5807.9127\n",
            "Epoch 37/100 | Loss: 5805.0100\n",
            "Epoch 38/100 | Loss: 5815.2124\n",
            "Epoch 39/100 | Loss: 5817.5313\n",
            "Epoch 40/100 | Loss: 5779.4809\n",
            "Epoch 41/100 | Loss: 5757.5244\n",
            "Epoch 42/100 | Loss: 5777.6289\n",
            "Epoch 43/100 | Loss: 5753.0446\n",
            "Epoch 44/100 | Loss: 5777.4521\n",
            "Epoch 45/100 | Loss: 5792.9062\n",
            "Epoch 46/100 | Loss: 5793.7593\n",
            "Epoch 47/100 | Loss: 5771.2859\n",
            "Epoch 48/100 | Loss: 5798.2585\n",
            "Epoch 49/100 | Loss: 5781.4233\n",
            "Epoch 50/100 | Loss: 5806.7860\n",
            "Epoch 51/100 | Loss: 5771.5813\n",
            "Epoch 52/100 | Loss: 5804.8285\n",
            "Epoch 53/100 | Loss: 5768.5829\n",
            "Epoch 54/100 | Loss: 5802.2829\n",
            "Epoch 55/100 | Loss: 5765.3950\n",
            "Epoch 56/100 | Loss: 5800.6444\n",
            "Epoch 57/100 | Loss: 5762.6466\n",
            "Epoch 58/100 | Loss: 5798.7213\n",
            "Epoch 59/100 | Loss: 5759.7132\n",
            "Epoch 60/100 | Loss: 5796.7506\n",
            "Epoch 61/100 | Loss: 5756.7347\n",
            "Epoch 62/100 | Loss: 5794.6571\n",
            "Epoch 63/100 | Loss: 5753.6630\n",
            "Epoch 64/100 | Loss: 5792.8561\n",
            "Epoch 65/100 | Loss: 5750.7251\n",
            "Epoch 66/100 | Loss: 5790.9922\n",
            "Epoch 67/100 | Loss: 5747.7081\n",
            "Epoch 68/100 | Loss: 5789.0666\n",
            "Epoch 69/100 | Loss: 5744.6269\n",
            "Epoch 70/100 | Loss: 5787.3339\n",
            "Epoch 71/100 | Loss: 5741.6108\n",
            "Epoch 72/100 | Loss: 5785.5617\n",
            "Epoch 73/100 | Loss: 5738.5202\n",
            "Epoch 74/100 | Loss: 5783.8476\n",
            "Epoch 75/100 | Loss: 5735.4296\n",
            "Epoch 76/100 | Loss: 5782.2322\n",
            "Epoch 77/100 | Loss: 5732.3428\n",
            "Epoch 78/100 | Loss: 5780.7452\n",
            "Epoch 79/100 | Loss: 5729.2784\n",
            "Epoch 80/100 | Loss: 5779.3896\n",
            "Epoch 81/100 | Loss: 5726.2182\n",
            "Epoch 82/100 | Loss: 5778.2418\n",
            "Epoch 83/100 | Loss: 5699.2949\n",
            "Epoch 84/100 | Loss: 5673.8306\n",
            "Epoch 85/100 | Loss: 5686.3123\n",
            "Epoch 86/100 | Loss: 5636.3096\n",
            "Epoch 87/100 | Loss: 5688.3239\n",
            "Epoch 88/100 | Loss: 5627.3621\n",
            "Epoch 89/100 | Loss: 5681.2599\n",
            "Epoch 90/100 | Loss: 5609.8051\n",
            "Epoch 91/100 | Loss: 5633.8916\n",
            "Epoch 92/100 | Loss: 5610.9202\n",
            "Epoch 93/100 | Loss: 5628.9125\n",
            "Epoch 94/100 | Loss: 5607.0585\n",
            "Epoch 95/100 | Loss: 5643.7969\n",
            "Epoch 96/100 | Loss: 5626.9258\n",
            "Epoch 97/100 | Loss: 5668.0427\n",
            "Epoch 98/100 | Loss: 5570.5188\n",
            "Epoch 99/100 | Loss: 5602.4511\n",
            "Epoch 100/100 | Loss: 5616.9109\n",
            "\n",
            "Treinando Nigeria Akure - DNI | opt=sgd | lr=0.0001\n",
            "Epoch 1/100 | Loss: 11909.5758\n",
            "Epoch 2/100 | Loss: 5796.1369\n",
            "Epoch 3/100 | Loss: 5797.3828\n",
            "Epoch 4/100 | Loss: 5791.0541\n",
            "Epoch 5/100 | Loss: 5796.5708\n",
            "Epoch 6/100 | Loss: 5789.6209\n",
            "Epoch 7/100 | Loss: 5795.4100\n",
            "Epoch 8/100 | Loss: 5788.5454\n",
            "Epoch 9/100 | Loss: 5794.3765\n",
            "Epoch 10/100 | Loss: 5787.5202\n",
            "Epoch 11/100 | Loss: 5793.3573\n",
            "Epoch 12/100 | Loss: 5786.4941\n",
            "Epoch 13/100 | Loss: 5792.3380\n",
            "Epoch 14/100 | Loss: 5785.4672\n",
            "Epoch 15/100 | Loss: 5791.3172\n",
            "Epoch 16/100 | Loss: 5784.4373\n",
            "Epoch 17/100 | Loss: 5790.2945\n",
            "Epoch 18/100 | Loss: 5783.4046\n",
            "Epoch 19/100 | Loss: 5789.2699\n",
            "Epoch 20/100 | Loss: 5782.3701\n",
            "Epoch 21/100 | Loss: 5788.2427\n",
            "Epoch 22/100 | Loss: 5781.3300\n",
            "Epoch 23/100 | Loss: 5787.2122\n",
            "Epoch 24/100 | Loss: 5780.2870\n",
            "Epoch 25/100 | Loss: 5786.1782\n",
            "Epoch 26/100 | Loss: 5779.2387\n",
            "Epoch 27/100 | Loss: 5785.1387\n",
            "Epoch 28/100 | Loss: 5778.1836\n",
            "Epoch 29/100 | Loss: 5784.0973\n",
            "Epoch 30/100 | Loss: 5777.1259\n",
            "Epoch 31/100 | Loss: 5783.0499\n",
            "Epoch 32/100 | Loss: 5776.0608\n",
            "Epoch 33/100 | Loss: 5781.9953\n",
            "Epoch 34/100 | Loss: 5774.9864\n",
            "Epoch 35/100 | Loss: 5780.9370\n",
            "Epoch 36/100 | Loss: 5773.9081\n",
            "Epoch 37/100 | Loss: 5779.8709\n",
            "Epoch 38/100 | Loss: 5772.8193\n",
            "Epoch 39/100 | Loss: 5778.7959\n",
            "Epoch 40/100 | Loss: 5771.7193\n",
            "Epoch 41/100 | Loss: 5777.7153\n",
            "Epoch 42/100 | Loss: 5770.6141\n",
            "Epoch 43/100 | Loss: 5776.6243\n",
            "Epoch 44/100 | Loss: 5769.4931\n",
            "Epoch 45/100 | Loss: 5775.5224\n",
            "Epoch 46/100 | Loss: 5768.3596\n",
            "Epoch 47/100 | Loss: 5774.4126\n",
            "Epoch 48/100 | Loss: 5767.2159\n",
            "Epoch 49/100 | Loss: 5773.2878\n",
            "Epoch 50/100 | Loss: 5766.0494\n",
            "Epoch 51/100 | Loss: 5772.1520\n",
            "Epoch 52/100 | Loss: 5764.8674\n",
            "Epoch 53/100 | Loss: 5770.9939\n",
            "Epoch 54/100 | Loss: 5763.6471\n",
            "Epoch 55/100 | Loss: 5769.7970\n",
            "Epoch 56/100 | Loss: 5762.3605\n",
            "Epoch 57/100 | Loss: 5768.4351\n",
            "Epoch 58/100 | Loss: 5760.8240\n",
            "Epoch 59/100 | Loss: 5765.8666\n",
            "Epoch 60/100 | Loss: 5757.3766\n",
            "Epoch 61/100 | Loss: 5760.7782\n",
            "Epoch 62/100 | Loss: 5750.6917\n",
            "Epoch 63/100 | Loss: 5762.6685\n",
            "Epoch 64/100 | Loss: 5750.4683\n",
            "Epoch 65/100 | Loss: 5762.2011\n",
            "Epoch 66/100 | Loss: 5749.7654\n",
            "Epoch 67/100 | Loss: 5761.4267\n",
            "Epoch 68/100 | Loss: 5748.8534\n",
            "Epoch 69/100 | Loss: 5760.4963\n",
            "Epoch 70/100 | Loss: 5747.8347\n",
            "Epoch 71/100 | Loss: 5759.4862\n",
            "Epoch 72/100 | Loss: 5746.7600\n",
            "Epoch 73/100 | Loss: 5758.4333\n",
            "Epoch 74/100 | Loss: 5745.6545\n",
            "Epoch 75/100 | Loss: 5757.3559\n",
            "Epoch 76/100 | Loss: 5744.5286\n",
            "Epoch 77/100 | Loss: 5756.2607\n",
            "Epoch 78/100 | Loss: 5743.3874\n",
            "Epoch 79/100 | Loss: 5755.1527\n",
            "Epoch 80/100 | Loss: 5742.2332\n",
            "Epoch 81/100 | Loss: 5754.0332\n",
            "Epoch 82/100 | Loss: 5741.0676\n",
            "Epoch 83/100 | Loss: 5752.9026\n",
            "Epoch 84/100 | Loss: 5739.8898\n",
            "Epoch 85/100 | Loss: 5751.7600\n",
            "Epoch 86/100 | Loss: 5738.6994\n",
            "Epoch 87/100 | Loss: 5750.6056\n",
            "Epoch 88/100 | Loss: 5737.4967\n",
            "Epoch 89/100 | Loss: 5749.4393\n",
            "Epoch 90/100 | Loss: 5736.2809\n",
            "Epoch 91/100 | Loss: 5748.2604\n",
            "Epoch 92/100 | Loss: 5735.0518\n",
            "Epoch 93/100 | Loss: 5747.0685\n",
            "Epoch 94/100 | Loss: 5733.8086\n",
            "Epoch 95/100 | Loss: 5745.8626\n",
            "Epoch 96/100 | Loss: 5732.5506\n",
            "Epoch 97/100 | Loss: 5744.6422\n",
            "Epoch 98/100 | Loss: 5731.2776\n",
            "Epoch 99/100 | Loss: 5743.4074\n",
            "Epoch 100/100 | Loss: 5729.9892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def criar_plot(reals, preds, intervalo, xlabel, ylabel, marker, mdl, opt_name, lr):\n",
        "\n",
        "  plt.figure(figsize=(12, 5))\n",
        "\n",
        "  if marker:\n",
        "    plt.plot(intervalo, reals, label=\"Real Data\", linewidth=0.9, color='black', marker='o')\n",
        "    plt.plot(intervalo, preds, label=\"Predio\", linewidth=0.9, color='red', marker='+')\n",
        "  else:\n",
        "    plt.plot(intervalo, reals, label=\"Real Data\", linewidth=0.9, color='black')\n",
        "    plt.plot(intervalo, preds, label=\"Predio\", linewidth=0.9, color='red')\n",
        "\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.title(\"Real vs Predicted\")\n",
        "  plt.grid(True, alpha=0.3)\n",
        "  plt.legend()\n",
        "  plt.tight_layout()\n",
        "\n",
        "  os.makedirs('output_plots', exist_ok=True)\n",
        "  save_path = os.path.join('output_plots', f\"{mdl['cidade']}_{mdl['target']}_{opt_name}_{lr}.png\")\n",
        "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "  plt.show()\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "lu0zxar-eOCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "metricas = []\n",
        "\n",
        "for md in models_data:\n",
        "  for opt_name in OTIMIZADORES:\n",
        "    for lr in LRS:\n",
        "\n",
        "      caminho = f\"output/{md['cidade']}_{md['target']}_{opt_name}_{lr}.pth\"\n",
        "\n",
        "      if not os.path.exists(caminho):\n",
        "        print(f\"Modelo no encontrado: {caminho}\")\n",
        "        continue\n",
        "\n",
        "      model = Model(\n",
        "          num_features=md[\"qnt_features\"],\n",
        "          num_outputs=1,\n",
        "          neurons=md[\"params\"][\"neurons\"],\n",
        "          dropouts=md[\"params\"][\"dropouts\"]\n",
        "      )\n",
        "\n",
        "      model.load_state_dict(torch.load(caminho))\n",
        "      model.eval()\n",
        "\n",
        "      preds, reals = [], []\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for Xb, yb in md[\"test_loader\"]:\n",
        "              pred = model(Xb).cpu().numpy().reshape(-1)\n",
        "              preds.append(pred)\n",
        "              reals.append(yb.cpu().numpy().reshape(-1))\n",
        "\n",
        "      preds = np.concatenate(preds).reshape(-1, 1)\n",
        "      reals = np.concatenate(reals).reshape(-1, 1)\n",
        "\n",
        "      # DESNORMALIZAO AQUI\n",
        "      # scaler_y = md[\"scaler_y\"]\n",
        "      # preds = scaler_y.inverse_transform(preds).reshape(-1)\n",
        "      # reals = scaler_y.inverse_transform(reals).reshape(-1)\n",
        "\n",
        "      # print(\"NaNs reals:\", np.isnan(reals).any())\n",
        "      # print(\"NaNs preds:\", np.isnan(preds).any())\n",
        "      # print(\"std reals:\", np.std(reals))\n",
        "      # print(\"std preds:\", np.std(preds))\n",
        "\n",
        "      mae = mean_absolute_error(reals, preds)\n",
        "      rmse = np.sqrt(mean_squared_error(reals, preds))\n",
        "      #r = np.corrcoef(reals, preds)[0,1] ** 2\n",
        "      r = np.corrcoef(reals.reshape(-1), preds.reshape(-1))[0, 1] ** 2\n",
        "\n",
        "      print(md[\"cidade\"], md[\"target\"], opt_name, lr, \": \", mae, rmse, r)\n",
        "\n",
        "      metricas.append({\n",
        "          \"cidade\": md[\"cidade\"],\n",
        "          \"target\": md[\"target\"],\n",
        "          \"optimizer\": opt_name,\n",
        "          \"lr\": lr,\n",
        "          \"MAE\": mae,\n",
        "          \"RMSE\": rmse,\n",
        "          \"R\": r\n",
        "      })\n",
        "\n",
        "      dataset_type = md['tipo']\n",
        "      target = md['target']\n",
        "\n",
        "      if dataset_type == 'wb':\n",
        "        s, f = 9_500, 10_500\n",
        "\n",
        "        if md['cidade'] == 'Touba':\n",
        "          s += 720\n",
        "          f += 720\n",
        "\n",
        "        reals = reals[s: f]\n",
        "        preds = preds[s: f]\n",
        "        intervalo = list(range(s, f))\n",
        "        xlabel = 'Time [min]'\n",
        "        marker = False\n",
        "\n",
        "        if target == 'dhi_rsi':\n",
        "          ylabel = 'Diffused Horizontal Irradiance in W/m'\n",
        "        elif target == 'ghi_pyr':\n",
        "          ylabel = 'Global Horizontal Irradiance from thermopile pyranometer in W/m'\n",
        "        elif target == 'ghi_sil':\n",
        "          ylabel = 'Global Horizontal Irradiance from silicon pyranometer in W/m'\n",
        "\n",
        "      elif dataset_type == 'tmy':\n",
        "        s, f = 5000, 5070\n",
        "        reals = reals[s:f]\n",
        "        preds = preds[s:f]\n",
        "        intervalo = list(range(s, f))\n",
        "        xlabel = 'Time [h]'\n",
        "        marker = True\n",
        "\n",
        "        if target == 'GSR':\n",
        "          ylabel = 'Global beam direct solar irradiance in W/m'\n",
        "        elif target == 'DSR':\n",
        "          ylabel = 'Diffused solar irradiance in W/m2'\n",
        "\n",
        "      elif dataset_type == 'sarah':\n",
        "        s, f = 0, 1_140\n",
        "        reals = reals[s:f]\n",
        "        preds = preds[s:f]\n",
        "        intervalo = list(range(s, f))\n",
        "        xlabel = 'Timestep [Days]'\n",
        "        marker = False\n",
        "\n",
        "        ylabel = 'Direct Normal Irradiance [W/m]'\n",
        "\n",
        "\n",
        "\n",
        "      criar_plot(reals, preds, intervalo, xlabel, ylabel, marker, md, opt_name, lr)\n"
      ],
      "metadata": {
        "id": "MIznuH9mVCbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_metricas = pd.DataFrame(metricas)\n",
        "df_metricas.to_csv(\"metricas_modelos.csv\", index=False)"
      ],
      "metadata": {
        "id": "UHi1Jyo3WQor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "shutil.make_archive(\"output_plots\", \"zip\", \"output_plots\")\n",
        "files.download(\"output_plots.zip\")\n",
        "\n",
        "shutil.make_archive(\"output\", \"zip\", \"output\")\n",
        "files.download(\"output.zip\")\n",
        "\n",
        "files.download('metricas_modelos.csv')"
      ],
      "metadata": {
        "id": "mynPMP0sXpsr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d2670c5b-6567-431b-e3bf-78cf2aa57221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_28ef1517-f2f2-4732-9e4d-aeb0f4036232\", \"metricas_modelos.csv\", 1050)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from openpyxl import load_workbook\n",
        "from openpyxl.utils import get_column_letter\n",
        "\n",
        "\n",
        "df_metricas.to_excel(\"resultado_formatado.xlsx\", index=False)\n",
        "\n",
        "wb = load_workbook(\"resultado_formatado.xlsx\")\n",
        "ws = wb.active\n",
        "\n",
        "# Mescla Touba (linhas 24)\n",
        "ws.merge_cells(\"A2:A4\")\n",
        "\n",
        "# Mescla Fatick (linhas 57)\n",
        "ws.merge_cells(\"A5:A7\")\n",
        "\n",
        "wb.save(\"resultado_formatado.xlsx\")\n",
        "files.download('resultado_formatado.xlsx')\n"
      ],
      "metadata": {
        "id": "1x-rerSS-7Sa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "97c7ec41-4fd6-4916-c7fa-057240caccd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_df6f6325-ea42-4dde-a397-431bb79156a4\", \"resultado_formatado.xlsx\", 5804)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}