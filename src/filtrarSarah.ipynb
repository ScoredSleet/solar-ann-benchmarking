{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4e82a1b",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5c5c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dbbfbd",
   "metadata": {},
   "source": [
    "*Transformando o Data Set Sarah 3.0 para Pandas*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "873ecd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processarLatitudeLongitudeArquivoNC(caminho: str, destino: str, latitude_alvo: xr.DataArray, longitude_alvo: xr.DataArray):\n",
    "    \"\"\"\n",
    "    Lê um arquivo NetCDF, filtra por uma lista de coordenadas (pontos específicos)\n",
    "    usando o método 'nearest' e salva o resultado em um novo arquivo NetCDF.\n",
    "\n",
    "    Args:\n",
    "        caminho (str): O caminho completo do arquivo .nc de entrada.\n",
    "        destino (str): O caminho completo onde o arquivo .nc de saída será salvo (incluindo o nome do arquivo).\n",
    "        latitude (List[float]): Lista de latitudes dos pontos desejados.\n",
    "        longitude (List[float]): Lista de longitudes correspondentes aos pontos desejados.\n",
    "\n",
    "    Returns:\n",
    "        None: A função não retorna valor, apenas gera o arquivo no disco.\n",
    "    \"\"\"\n",
    "\n",
    "    with xr.open_dataset(caminho) as data_set:\n",
    "        resultado = data_set.sel(lat=latitude_alvo, lon=longitude_alvo, method='nearest')\n",
    "\n",
    "        pasta_destino = os.path.dirname(destino)\n",
    "        if pasta_destino and not os.path.exists(pasta_destino):\n",
    "            return\n",
    "\n",
    "        resultado.to_netcdf(destino)\n",
    "\n",
    "def processarPastaNC(caminho: str, destino: str,  latitude: List[float], longitude: List[float]):\n",
    "    \"\"\"\n",
    "    Varre uma pasta inteira buscando arquivos .nc e processa todos eles em lote.\n",
    "\n",
    "    Args:\n",
    "        caminho (str): O diretório (pasta) onde estão os arquivos originais.\n",
    "        destino (str): O diretório (pasta) onde os arquivos processados serão salvos.\n",
    "        latitude (List[float]): Lista de latitudes a serem filtradas em todos os arquivos.\n",
    "        longitude (List[float]): Lista de longitudes a serem filtradas em todos os arquivos.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(destino):\n",
    "        os.makedirs(destino)\n",
    "    \n",
    "    padrao = os.path.join(caminho, \"*.nc\")\n",
    "    arquivos_nc = glob.glob(padrao)\n",
    "\n",
    "    latitude_alvo  = xr.DataArray(latitude, dims=\"pontos\")\n",
    "    longitude_alvo = xr.DataArray(longitude, dims=\"pontos\")\n",
    "    for indice, caminho_arquivo_atual in enumerate(arquivos_nc):\n",
    "        \n",
    "        nome_arquivo = os.path.basename(caminho_arquivo_atual)\n",
    "        \n",
    "        # Cria o novo nome (ex: \"saida/dados_2024_processado.nc\")\n",
    "        nome_novo = f\"dataset_{indice + 1:05d}.nc\"\n",
    "        caminho_destino = os.path.join(destino, nome_novo)\n",
    "        try: \n",
    "            \n",
    "            if indice % 1000 == 0:\n",
    "                print(f\"Processando {indice + 1}/{len(arquivos_nc)}\")\n",
    "\n",
    "            processarLatitudeLongitudeArquivoNC(\n",
    "                caminho=caminho_arquivo_atual,\n",
    "                destino=caminho_destino,\n",
    "                latitude_alvo=latitude_alvo, \n",
    "                longitude_alvo=longitude_alvo\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar {nome_arquivo}: {e}\")\n",
    "\n",
    "def converterNCparaCSV(caminho: str, destino: str):\n",
    "    padrao = os.path.join(caminho, \"*.nc\")\n",
    "    arquivos_nc = glob.glob(padrao)\n",
    "\n",
    "    if not arquivos_nc:\n",
    "        print(\"Nenhum arquivo .nc encontrado na pasta!\")\n",
    "        return\n",
    "    tam = len(arquivos_nc)\n",
    "\n",
    "    lista_de_tabelas = []\n",
    "    for indice, arquivo in enumerate(arquivos_nc):\n",
    "        if indice % 1000 == 0:\n",
    "            print(f\"{indice} / {tam}\")\n",
    "        try:\n",
    "            with xr.open_dataset(arquivo) as dataset:\n",
    "                dataframe_temp = dataset.to_dataframe()\n",
    "                dataframe_temp = dataframe_temp.reset_index()\n",
    "                lista_de_tabelas.append(dataframe_temp)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler {arquivo}: {e}\")\n",
    "            return\n",
    "    if lista_de_tabelas:\n",
    "        df_final = pd.concat(lista_de_tabelas, ignore_index=True)\n",
    "        df_final.to_csv(destino, index=True, sep=';')\n",
    "\n",
    "        return\n",
    "    else:\n",
    "        print(\"Nenhum dado foi processado.\")\n",
    "        return\n",
    "\n",
    "def filtrar_ponto_com_tolerancia(df, lat_alvo, lon_alvo, tol=0.01):\n",
    "    \"\"\"\n",
    "    Retorna apenas as linhas que dão match com a lat E lon ao mesmo tempo.\n",
    "    \"\"\"\n",
    "    condicao_lat = np.isclose(df['lat'], lat_alvo, atol=tol)\n",
    "    condicao_lon = np.isclose(df['lon'], lon_alvo, atol=tol)\n",
    "    \n",
    "    # O símbolo '&' significa E (tem que satisfazer as duas condições)\n",
    "    return df[condicao_lat & condicao_lon]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f829308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando 1/12785\n",
      "Processando 1001/12785\n",
      "Processando 2001/12785\n",
      "Processando 3001/12785\n",
      "Processando 4001/12785\n",
      "Processando 5001/12785\n",
      "Processando 6001/12785\n",
      "Processando 7001/12785\n",
      "Processando 8001/12785\n",
      "Processando 9001/12785\n",
      "Processando 10001/12785\n",
      "Processando 11001/12785\n",
      "Processando 12001/12785\n",
      "0 / 12785\n",
      "1000 / 12785\n",
      "2000 / 12785\n",
      "3000 / 12785\n",
      "4000 / 12785\n",
      "5000 / 12785\n",
      "6000 / 12785\n",
      "7000 / 12785\n",
      "8000 / 12785\n",
      "9000 / 12785\n",
      "10000 / 12785\n",
      "11000 / 12785\n",
      "12000 / 12785\n"
     ]
    }
   ],
   "source": [
    "lat = [\n",
    "    24.072,   # Algeria Tamarasset\n",
    "    11.908,   # Nigeria Borno\n",
    "    9.826,    # CAR Vakaga\n",
    "    9.0723,   # Nigeria Abuja\n",
    "    14.773,   # Senegal Touba\n",
    "    7.25,     # Nigeria Akure\n",
    "    24.475,   # Egypt Mut\n",
    "    14.3675,  # Senegal Fatick\n",
    "    -29.186   # South Africa Northern Cape\n",
    "]\n",
    "\n",
    "# Array de Longitudes (Eixo X)\n",
    "lon = [\n",
    "    4.679,    # Algeria Tamarasset\n",
    "    13.427,   # Nigeria Borno\n",
    "    22.508,   # CAR Vakaga\n",
    "    7.4913,   # Nigeria Abuja\n",
    "    -15.9196, # Senegal Touba\n",
    "    5.19,     # Nigeria Akure\n",
    "    28.466,   # Egypt Mut\n",
    "    -16.4135, # Senegal Fatick\n",
    "    20.464    # South Africa Northern Cape\n",
    "]\n",
    "\n",
    "processarPastaNC(\"./SARAH/ORD63865\", \"./SARAH/filtrado\", latitude=lat, longitude=lon)\n",
    "\n",
    "converterNCparaCSV(\"./SARAH/filtrado\", \"./SARAH/dataset_sarah_SD.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4f1c5e",
   "metadata": {},
   "source": [
    "**Juntando as duas tabelas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "154e5f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualização das 5 primeiras linhas dos dados processados:\n",
      "        time  year  month  day  lat_dni  lon_dni        SDU    DNI\n",
      "0 1983-01-01  1983      1    1   24.075    4.675  10.033001  290.0\n",
      "1 1983-01-01  1983      1    1   11.925   13.425   9.840000  327.0\n",
      "2 1983-01-01  1983      1    1    9.825   22.525  10.433001  282.0\n",
      "3 1983-01-01  1983      1    1    9.075    7.475   9.883000  296.0\n",
      "4 1983-01-01  1983      1    1   14.775  -15.925   9.574000  322.0\n",
      "        time  lat_dni  lon_dni       Location_Name\n",
      "0 1983-01-01   24.075    4.675  Algeria_Tamarasset\n",
      "1 1983-01-01   11.925   13.425       Nigeria_Borno\n",
      "2 1983-01-01    9.825   22.525          CAR_Vakaga\n",
      "3 1983-01-01    9.075    7.475       Nigeria_Abuja\n",
      "4 1983-01-01   14.775  -15.925       Senegal_Touba\n"
     ]
    }
   ],
   "source": [
    "# 1. Carregar os arquivos (usando o separador ';')\n",
    "df_dni = pd.read_csv('./SARAH/dataset_sarah_DNI.csv', sep=';')\n",
    "df_sd = pd.read_csv('./SARAH/dataset_sarah_SD.csv', sep=';')\n",
    "\n",
    "# 2. Realizar a fusão (Merge)\n",
    "# Usamos 'time', 'pontos' e 'bnds' como chave para evitar duplicação incorreta\n",
    "df_merged = pd.merge(\n",
    "    df_dni, \n",
    "    df_sd, \n",
    "    on=['time', 'pontos', 'bnds'], \n",
    "    suffixes=('_dni', '_sd')\n",
    ")\n",
    "\n",
    "# 3. Limpeza e Engenharia de Features\n",
    "# Converter coluna de tempo para datetime\n",
    "df_merged['time'] = pd.to_datetime(df_merged['time'])\n",
    "\n",
    "# Extrair Mês e Dia (importantes para sazonalidade solar)\n",
    "df_merged['year']  = df_merged['time'].dt.year\n",
    "df_merged['month'] = df_merged['time'].dt.month\n",
    "df_merged['day']   = df_merged['time'].dt.day\n",
    "\n",
    "# Selecionar colunas finais\n",
    "# Entradas (X): SDU (Insolação), Latitude, Longitude, Mês, Dia\n",
    "# Saída (y): DNI (Irradiância Direta)\n",
    "cols_final = ['time', 'year', 'month', 'day', 'lat_dni', 'lon_dni', 'SDU', 'DNI']\n",
    "df_final = df_merged[cols_final].dropna() # Remove linhas com valores vazios (NaN)\n",
    "\n",
    "# Renomear para facilitar\n",
    "cols_final = ['time', 'year', 'month', 'day', 'lat', 'lon', 'SDU', 'DNI']\n",
    "\n",
    "# 4. Visualização\n",
    "print(\"Visualização das 5 primeiras linhas dos dados processados:\")\n",
    "print(df_final.head())\n",
    "\n",
    "df_final.to_csv('./SARAH/dados_processados_sarah.csv', sep=';', index=False)\n",
    "\n",
    "# 2. Criar a tabela de referência com os dados que você passou\n",
    "ref_data = {\n",
    "    'Ref_Lat': [24.072, 11.908, 9.826, 9.0723, 14.773, 7.25, 24.475, 14.3675, -29.186],\n",
    "    'Ref_Lon': [4.679, 13.427, 22.508, 7.4913, -15.9196, 5.19, 28.466, -16.4135, 20.464],\n",
    "    'Location': [\n",
    "        'Algeria_Tamarasset', 'Nigeria_Borno', 'CAR_Vakaga', 'Nigeria_Abuja',\n",
    "        'Senegal_Touba', 'Nigeria_Akure', 'Egypt_Mut', 'Senegal_Fatick', 'South_Africa_Northern_Cape'\n",
    "    ]\n",
    "}\n",
    "df_ref = pd.DataFrame(ref_data)\n",
    "\n",
    "# 3. Função para encontrar o local mais próximo\n",
    "def get_location_name(row):\n",
    "    # Calcula a distância euclidiana para todos os pontos de referência\n",
    "    distances = np.sqrt(\n",
    "        (df_ref['Ref_Lat'] - row['lat_dni'])**2 + \n",
    "        (df_ref['Ref_Lon'] - row['lon_dni'])**2\n",
    "    )\n",
    "    # Pega o índice da menor distância\n",
    "    nearest_idx = distances.idxmin()\n",
    "    return df_ref.loc[nearest_idx, 'Location']\n",
    "\n",
    "# 4. Aplicar a função para criar a nova coluna\n",
    "# Primeiro pegamos as combinações únicas para ser mais rápido\n",
    "unique_coords = df_final[['lat_dni', 'lon_dni']].drop_duplicates()\n",
    "unique_coords['Location_Name'] = unique_coords.apply(get_location_name, axis=1)\n",
    "\n",
    "# Juntamos de volta ao dataframe principal\n",
    "df_mapped = pd.merge(df_final, unique_coords, on=['lat_dni', 'lon_dni'], how='left')\n",
    "\n",
    "# Exibir resultado\n",
    "print(df_mapped[['time', 'lat_dni', 'lon_dni', 'Location_Name']].head())\n",
    "\n",
    "# Salvar\n",
    "df_mapped.to_csv('./SARAH/dados_processados_com_locais.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c440b740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo o arquivo: ./dataset/dados_processados_com_locais.csv...\n",
      "Encontrados 9 locais únicos: ['Algeria_Tamarasset' 'Nigeria_Borno' 'CAR_Vakaga' 'Nigeria_Abuja'\n",
      " 'Senegal_Touba' 'Nigeria_Akure' 'Egypt_Mut' 'Senegal_Fatick'\n",
      " 'South_Africa_Northern_Cape']\n",
      "-> Salvo: datasets_separados\\SARAH_Algeria_Tamarasset.csv (22830 linhas)\n",
      "-> Salvo: datasets_separados\\SARAH_Nigeria_Borno.csv (22834 linhas)\n",
      "-> Salvo: datasets_separados\\SARAH_CAR_Vakaga.csv (22844 linhas)\n",
      "-> Salvo: datasets_separados\\SARAH_Nigeria_Abuja.csv (22832 linhas)\n",
      "-> Salvo: datasets_separados\\SARAH_Senegal_Touba.csv (22810 linhas)\n",
      "-> Salvo: datasets_separados\\SARAH_Nigeria_Akure.csv (22826 linhas)\n",
      "-> Salvo: datasets_separados\\SARAH_Egypt_Mut.csv (22852 linhas)\n",
      "-> Salvo: datasets_separados\\SARAH_Senegal_Fatick.csv (22808 linhas)\n",
      "-> Salvo: datasets_separados\\SARAH_South_Africa_Northern_Cape.csv (22850 linhas)\n",
      "\n",
      "Processo concluído com sucesso!\n"
     ]
    }
   ],
   "source": [
    "def separar_dataset_por_local(caminho_arquivo: str, coluna_local: str = 'Location_Name'):\n",
    "    \"\"\"\n",
    "    Lê um arquivo CSV e salva arquivos separados para cada valor único\n",
    "    encontrado na coluna especificada.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Carregar o dataset principal\n",
    "        # O parâmetro sep=';' é importante pois seu arquivo usa ponto e vírgula\n",
    "        print(f\"Lendo o arquivo: {caminho_arquivo}...\")\n",
    "        df = pd.read_csv(caminho_arquivo, sep=';')\n",
    "        \n",
    "        # Verifica se a coluna existe\n",
    "        if coluna_local not in df.columns:\n",
    "            print(f\"Erro: Coluna '{coluna_local}' não encontrada no arquivo.\")\n",
    "            return\n",
    "\n",
    "        # 2. Identificar os locais únicos\n",
    "        locais_unicos = df[coluna_local].unique()\n",
    "        print(f\"Encontrados {len(locais_unicos)} locais únicos: {locais_unicos}\")\n",
    "\n",
    "        # Criar uma pasta para os outputs (opcional, para organização)\n",
    "        pasta_saida = \"datasets_separados\"\n",
    "        os.makedirs(pasta_saida, exist_ok=True)\n",
    "\n",
    "        # 3. Loop para filtrar e salvar cada arquivo\n",
    "        for local in locais_unicos:\n",
    "            # Filtra apenas as linhas desse local\n",
    "            df_local = df[df[coluna_local] == local]\n",
    "            \n",
    "            # Limpa o nome do local para usar no arquivo (remove caracteres ruins se houver)\n",
    "            # Ex: \"South_Africa_Northern_Cape\" -> \"South_Africa_Northern_Cape\"\n",
    "            nome_seguro = str(local).replace(' ', '_').replace('/', '-')\n",
    "            \n",
    "            # Define o nome do arquivo\n",
    "            nome_arquivo = f\"SARAH_{nome_seguro}.csv\"\n",
    "            caminho_saida = os.path.join(pasta_saida, nome_arquivo)\n",
    "            \n",
    "            # Salva o arquivo (mantendo o separador ; para consistência)\n",
    "            df_local.to_csv(caminho_saida, sep=';', index=False)\n",
    "            print(f\"-> Salvo: {caminho_saida} ({len(df_local)} linhas)\")\n",
    "\n",
    "        print(\"\\nProcesso concluído com sucesso!\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erro: O arquivo '{caminho_arquivo}' não foi encontrado.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro inesperado: {e}\")\n",
    "\n",
    "arquivo_entrada = \"./dataset/dados_processados_com_locais.csv\"\n",
    "    \n",
    "separar_dataset_por_local(arquivo_entrada)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
