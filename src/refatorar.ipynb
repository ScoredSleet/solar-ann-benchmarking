{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9abf88c",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f725cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130408c6",
   "metadata": {},
   "source": [
    "**Classes do Modelo ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3975b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solar ANN\n",
    "\n",
    "class SolarANN(nn.Module):\n",
    "    def __init__(self, input_dim, neurons, dropouts, activation_function_per_layer):\n",
    "        \"\"\"\n",
    "        Constrói a rede neural dinamicamente baseada nas listas de neurônios e dropouts.\n",
    "        \"\"\"\n",
    "        super(SolarANN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        function = {\n",
    "            'relu'   : nn.ReLU(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'tanh'   : nn.Tanh()\n",
    "        }\n",
    "\n",
    "        # Cria as camadas ocultas dinamicamente\n",
    "        for out_dim, drop_rate, activation_function in zip(neurons, dropouts, activation_function_per_layer):\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            layers.append(function[activation_function])\n",
    "            if drop_rate > 0:\n",
    "                layers.append(nn.Dropout(drop_rate))\n",
    "            in_dim = out_dim\n",
    "            \n",
    "        layers.append(nn.Linear(in_dim, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "class SolarPrediction:\n",
    "    def __init__(self, nome_cidade, config):\n",
    "        self.nome = nome_cidade\n",
    "        self.config = config\n",
    "        self.df = None\n",
    "        self.scalers = {} # Para guardar o scaler de cada target se necessário\n",
    "        self.loaders = {}\n",
    "        self.input_dim = 0\n",
    "        \n",
    "        # Carrega os dados imediatamente ao instanciar\n",
    "        self._carregar_dados()\n",
    "\n",
    "    def _carregar_dados(self):\n",
    "        path = self.config[\"arquivo\"]\n",
    "        tipo = self.config[\"tipo\"]\n",
    "        \n",
    "        # Lógica encapsulada das funções anteriores\n",
    "        if tipo == \"wb\":\n",
    "            df = pd.read_csv(path, sep=';')\n",
    "            df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "            df['year'] = df['time'].dt.year\n",
    "            df['month'] = df['time'].dt.month\n",
    "            df['day'] = df['time'].dt.day\n",
    "            df['hour'] = df['time'].dt.hour\n",
    "            df['minute'] = df['time'].dt.minute\n",
    "\n",
    "            df.drop(columns=['time', 'comments'], inplace=True)\n",
    "\n",
    "            return df[\n",
    "                ['year','month','day','hour','minute','air_temperature',\n",
    "                'relative_humidity','wind_speed','wind_from_direction',\n",
    "                'wind_speed_calc','sensor_cleaning','precipitation',\n",
    "                'barometric_pressure','dhi_rsi','ghi_sil','ghi_pyr']\n",
    "            ]\n",
    "\n",
    "        elif tipo == \"tmy\":\n",
    "            df = pd.read_csv(\n",
    "                path, sep=',', skiprows=8, skipfooter=12, engine='python'\n",
    "            )\n",
    "\n",
    "            df['time'] = pd.to_datetime(df['time'], format=\"%Y%m%d:%H%M\")\n",
    "\n",
    "            df['year'] = df['time'].dt.year\n",
    "            df['month'] = df['time'].dt.month\n",
    "            df['day'] = df['time'].dt.day\n",
    "            df['hour'] = df['time'].dt.hour\n",
    "\n",
    "            df.drop(columns=['time','Int','Gr(i)'], inplace=True)\n",
    "\n",
    "            df = df.rename(columns={ 'Gb(i)': 'GSR', 'Gd(i)': 'DSR' })\n",
    "\n",
    "            return df[['year','month','day','hour','H_sun','T2m','WS10m','GSR','DSR']]\n",
    "            \n",
    "        elif tipo == \"sarah\":\n",
    "            df = pd.read_csv(path, sep=';')\n",
    "\n",
    "            df['time'] = pd.to_datetime(df['time'], format=\"%d/%m/%Y\")\n",
    "\n",
    "            df['year'] = df['time'].dt.year\n",
    "            df['month'] = df['time'].dt.month\n",
    "            df['day'] = df['time'].dt.day\n",
    "\n",
    "            df.drop(columns=['time'], inplace=True)\n",
    "\n",
    "            return df[['year','month','day','SDU','DNI']]\n",
    "            \n",
    "        self.df = self.df.dropna()\n",
    "\n",
    "    def preparar_dataloaders(self, target_col, batch_size):\n",
    "        \"\"\"\n",
    "        Prepara X e y, normaliza e cria os DataLoaders para um target específico.\n",
    "        \"\"\"\n",
    "        ignorar = self.config[\"ignorar\"]\n",
    "        \n",
    "        # Separa features e target\n",
    "        X = self.df.drop(columns=ignorar).values\n",
    "        y = self.df[target_col].values.reshape(-1, 1)\n",
    "\n",
    "        # Normalização\n",
    "        scaler_X = StandardScaler()\n",
    "        scaler_y = StandardScaler()\n",
    "        \n",
    "        X = scaler_X.fit_transform(X)\n",
    "        y = scaler_y.fit_transform(y)\n",
    "        \n",
    "        # Guarda o scaler para desnormalizar depois\n",
    "        self.scalers[target_col] = scaler_y\n",
    "        self.input_dim = X.shape[1]\n",
    "\n",
    "        # Split treino/teste (séries temporais não deve ter shuffle no split temporal)\n",
    "        n = len(self.df)\n",
    "        t = int(self.config['training'] * n)\n",
    "        \n",
    "        X_train = torch.tensor(X[:t], dtype=torch.float32)\n",
    "        y_train = torch.tensor(y[:t], dtype=torch.float32)\n",
    "        X_test = torch.tensor(X[t:], dtype=torch.float32)\n",
    "        y_test = torch.tensor(y[t:], dtype=torch.float32)\n",
    "\n",
    "        train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    def treinar_modelo(self, model, train_loader, epochs, lr, optimizer_name=\"adam\"):\n",
    "        \"\"\"\n",
    "        Executa o loop de treinamento para um modelo e loader específicos.\n",
    "        \"\"\"\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        if optimizer_name == \"adam\":\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        else:\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "        best_loss = np.inf\n",
    "        best_state = None\n",
    "        \n",
    "        print(f\"Iniciando treino para {self.nome}...\")\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            epoch_loss /= len(train_loader)\n",
    "            \n",
    "            if epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_state = model.state_dict()\n",
    "                \n",
    "            print(f\"Epoch {epoch+1}/{epochs} Loss: {epoch_loss:.4f}\")\n",
    "            \n",
    "        return best_state, best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb46a890",
   "metadata": {},
   "source": [
    "**Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8499b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main()\n",
    "    CIDADES = {\n",
    "    \"Touba\": {\n",
    "        \"arquivo\": \"solar-measurementssenegal-toubaifcqc.csv\",\n",
    "        \"tipo\": \"wb\",\n",
    "        \"targets\": [\"dhi_rsi\", \"ghi_pyr\", \"ghi_sil\"],\n",
    "        \"ignorar\": [\"dhi_rsi\", \"ghi_pyr\", \"ghi_sil\"],\n",
    "        \"neurons\": [[50, 50], [100, 200], [100, 50]],\n",
    "        \"dropouts\": [[0.25, 0.25], [0.25, 0.25], [0, 0]],\n",
    "        \"epochs\": [20, 40, 30],\n",
    "        \"batch_size\": [128, 128, 128]\n",
    "    },\n",
    "\n",
    "    \"Fatick\": {\n",
    "        \"arquivo\": \"solar-measurementssenegal-fatickifcqc.csv\",\n",
    "        \"tipo\": \"wb\",\n",
    "        \"targets\": [\"dhi_rsi\", \"ghi_pyr\", \"ghi_sil\"],\n",
    "        \"ignorar\": [\"dhi_rsi\", \"ghi_pyr\", \"ghi_sil\"],\n",
    "        \"neurons\": [[100, 50], [100, 50], [50, 50]],\n",
    "        \"dropouts\": [[0.25, 0.25], [0.0, 0.25], [0.25, 0.25]],\n",
    "        \"epochs\": [20, 50, 150],\n",
    "        \"batch_size\": [128, 128, 128]\n",
    "    },\n",
    "\n",
    "    \"SA Northern Cape\": {\n",
    "        \"arquivo\": \"Timeseries_SA_northern_cape_2005_2016.csv\",\n",
    "        \"tipo\": \"tmy\",\n",
    "        \"targets\": [\"GSR\"],\n",
    "        \"ignorar\": [\"GSR\", \"DSR\"],\n",
    "        \"neurons\": [[100, 50]],\n",
    "        \"dropouts\": [[0, 0]],\n",
    "        \"epochs\": [20],\n",
    "        \"batch_size\": [512]\n",
    "    },\n",
    "\n",
    "    \"CAR Vakaga\": {\n",
    "        \"arquivo\": \"Timeseries_CAR_vakaga_2005_2016.csv\",\n",
    "        \"tipo\": \"tmy\",\n",
    "        \"targets\": [\"GSR\"],\n",
    "        \"ignorar\": [\"GSR\", \"DSR\"],\n",
    "        \"neurons\": [[200, 200, 100]],\n",
    "        \"dropouts\": [[0, 0, 0]],\n",
    "        \"epochs\": [30],\n",
    "        \"batch_size\": [512]\n",
    "    },\n",
    "\n",
    "    \"Egypt Mut\": {\n",
    "        \"arquivo\": \"Timeseries_egypt_mut_2005_2016.csv\",\n",
    "        \"tipo\": \"tmy\",\n",
    "        \"targets\": [\"GSR\"],\n",
    "        \"ignorar\": [\"GSR\", \"DSR\"],\n",
    "        \"neurons\": [[200, 200, 100]],\n",
    "        \"dropouts\": [[0, 0, 0]],\n",
    "        \"epochs\": [7],\n",
    "        \"batch_size\": [128]\n",
    "    },\n",
    "\n",
    "    \"Algeria Tamanrasset\": {\n",
    "        \"arquivo\": \"Timeseries_tamaransset_2005_2016.csv\",\n",
    "        \"tipo\": \"tmy\",\n",
    "        \"targets\": [\"GSR\"],\n",
    "        \"ignorar\": [\"GSR\", \"DSR\"],\n",
    "        \"neurons\": [[100, 100, 50]],\n",
    "        \"dropouts\": [[0, 0, 0]],\n",
    "        \"epochs\": [100],\n",
    "        \"batch_size\": [512]\n",
    "    },\n",
    "\n",
    "    \"Nigeria Borno\": {\n",
    "        \"arquivo\": \"Timeseries_nigeria_borno_2005_2016.csv\",\n",
    "        \"tipo\": \"tmy\",\n",
    "        \"targets\": [\"DSR\"],\n",
    "        \"ignorar\": [\"GSR\", \"DSR\"],\n",
    "        \"neurons\": [[100, 50]],\n",
    "        \"dropouts\": [[0, 0]],\n",
    "        \"epochs\": [50],\n",
    "        \"batch_size\": [512]\n",
    "    },\n",
    "\n",
    "    \"Nigeria Abuja\": {\n",
    "        \"arquivo\": \"SARAH_nigeria_abuja.csv\",\n",
    "        \"tipo\": \"sarah\",\n",
    "        \"targets\": [\"DNI\"],\n",
    "        \"ignorar\": [\"DNI\"],\n",
    "        \"neurons\": [[200, 200, 50]],\n",
    "        \"dropouts\": [[0, 0, 0]],\n",
    "        \"epochs\": [100],\n",
    "        \"batch_size\": [128]\n",
    "    },\n",
    "\n",
    "    \"Nigeria Akure\": {\n",
    "        \"arquivo\": \"SARAH_nigeria_akure.csv\",\n",
    "        \"tipo\": \"sarah\",\n",
    "        \"targets\": [\"DNI\"],\n",
    "        \"ignorar\": [\"DNI\"],\n",
    "        \"neurons\": [[200, 200, 100]],\n",
    "        \"dropouts\": [[0, 0, 0]],\n",
    "        \"epochs\": [100],\n",
    "        \"batch_size\": [128]\n",
    "    }\n",
    "}\n",
    "\n",
    "OTIMIZADORES = [\"adam\", \"sgd\"]\n",
    "\n",
    "LRS = [1e-3, 5e-4, 1e-4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
